<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Vision Transformer | ʕ̯•͡˔•̯᷅ʔ</title><meta name="keywords" content="调研,Transformer,深度学习"><meta name="author" content="HouYuanyuan"><meta name="copyright" content="HouYuanyuan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一般使用的patch_size大小是16*16，可以通过减小patch_size来完成更精细化的tokenization，但是序列的长度N&#x3D;HW&#x2F;P^2过长，假设p从16变为4，序列的长度会变为原来的16倍，而ViT模型的计算量与序列长度的平方成正比，计算量就会相应地变成原来的256倍  1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下Attention结构，使">
<meta property="og:type" content="article">
<meta property="og:title" content="Vision Transformer">
<meta property="og:url" content="http://houyuanyuan.github.io/2021/07/10/011-Vision-Transformer/index.html">
<meta property="og:site_name" content="ʕ̯•͡˔•̯᷅ʔ">
<meta property="og:description" content="一般使用的patch_size大小是16*16，可以通过减小patch_size来完成更精细化的tokenization，但是序列的长度N&#x3D;HW&#x2F;P^2过长，假设p从16变为4，序列的长度会变为原来的16倍，而ViT模型的计算量与序列长度的平方成正比，计算量就会相应地变成原来的256倍  1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下Attention结构，使">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/a7feabd26c1f47ee84d517d0159dcab6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2021-07-10T15:36:14.000Z">
<meta property="article:modified_time" content="2022-07-05T16:00:00.000Z">
<meta property="article:author" content="HouYuanyuan">
<meta property="article:tag" content="调研">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/a7feabd26c1f47ee84d517d0159dcab6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70"><link rel="shortcut icon" href="/img/transparent.png"><link rel="canonical" href="http://houyuanyuan.github.io/2021/07/10/011-Vision-Transformer/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: HouYuanyuan","link":"链接: ","source":"来源: ʕ̯•͡˔•̯᷅ʔ","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Vision Transformer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-07-06 00:00:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/sun.png'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img-blog.csdnimg.cn/a7feabd26c1f47ee84d517d0159dcab6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ʕ̯•͡˔•̯᷅ʔ</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Vision Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-10T15:36:14.000Z" title="发表于 2021-07-10 23:36:14">2021-07-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-05T16:00:00.000Z" title="更新于 2022-07-06 00:00:00">2022-07-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E6%9C%9F%E9%97%B4/">研究生期间</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>21分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Vision Transformer"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>一般使用的patch_size大小是16*16，可以通过减小patch_size来完成更精细化的tokenization，但是序列的长度N=HW/P^2过长，假设p从16变为4，序列的长度会变为原来的16倍，而ViT模型的计算量与序列长度的平方成正比，计算量就会相应地变成原来的256倍</p>
<blockquote>
<p>1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下Attention结构，使得它有能力分辨不同位置的Token，这构成了相对位置编码的一般做法。<br>相对位置并没有完整建模每个输入的位置信息，而是在算Attention的时候考虑当前位置与被Attention的位置的相对距离，由于自然语言一般更依赖于相对位置，所以相对位置编码通常也有着优秀的表现。对于相对位置编码来说，它的灵活性更大，更加体现出了研究人员的“天马行空”。</p>
</blockquote>
<h2 id="2021-CoaT：Co-Scale-Conv-Attentional-ImageTransformers【开源】【分类】"><a href="#2021-CoaT：Co-Scale-Conv-Attentional-ImageTransformers【开源】【分类】" class="headerlink" title="2021-CoaT：Co-Scale Conv-Attentional ImageTransformers【开源】【分类】"></a>2021-CoaT：Co-Scale Conv-Attentional ImageTransformers【开源】【分类】</h2><p>【代码】： <a target="_blank" rel="noopener" href="https://github.com/mlpc-ucsd/CoaT">https://github.com/mlpc-ucsd/CoaT</a></p>
<p>CoaT为图像Transformer提供了丰富的多尺度和上下文建模能力</p>
<p><strong>摘要</strong><br>在本文中，我们介绍了Co-scale conv-attentional image Transformers（CoaT），这是一种基于Transformer的图像分类器，配备了co-scale和conv-attentional机制。首先，co-scale机制在各个尺度上都保持了Transformers编码器分支的完整性，同时允许在不同尺度下学习的表示形式能够有效地彼此通信。我们设计了一系列的串行和并行块，以实现co-scale 注意力机制。其次，我们通过一种高效的类似于卷积的实现方式，在因式注意模块中实现相对位置嵌入公式，设计了一种factorized attention机制。 CoaT为图像Trasformer提供了丰富的多尺度和上下文建模功能。在ImageNet上，与类似大小的卷积神经网络和图像/视觉Transformer相比，相对较小的CoaT模型可获得更好的分类结果。 CoaT骨干网的有效性在目标检测和实例分割上也得到了说明，证明了其对下游计算机视觉任务的适用性。</p>
<ol>
<li><img src="https://img-blog.csdnimg.cn/img_convert/e5c90f76658dacbdda4514770a973802.png" alt="在这里插入图片描述"><br>左边是CoaT-Lite，只有串行模块<br>右边是CoaT，加上并行模块</li>
</ol>
<h2 id="2021-CPVT：Conditional-Positional-Encodings-for-Vision-Transformer【开源】【分类】【美团】"><a href="#2021-CPVT：Conditional-Positional-Encodings-for-Vision-Transformer【开源】【分类】【美团】" class="headerlink" title="2021-CPVT：Conditional Positional Encodings for Vision Transformer【开源】【分类】【美团】"></a>2021-CPVT：Conditional Positional Encodings for Vision Transformer【开源】【分类】【美团】</h2><p>又名： Do We Really Need Explicit Position Encodings for Vision Transformers<br>【代码】： <a target="_blank" rel="noopener" href="https://github.com/Meituan-AutoML/CPVT">https://github.com/Meituan-AutoML/CPVT</a>   【代码是和Twins一起放出来的，这个链接没有代码】<br>成功的视觉任务位置编码应满足以下要求：</p>
<ol>
<li>是输入序列排列可变但平移不变</li>
<li>具有归纳性并且能够处理比训练期间更长的序列</li>
<li>具有一定程度能提供绝对位置的能力</li>
</ol>
<blockquote>
<p>CPVT的PEG（Positional Encoding Generator）通过卷积聚合邻域内的特征生成位置编码，且边缘的零填充对于模型知道绝对位置很重要，并且做了消融实验来验证，分析原因是因为：绝对位置信息在物体分类中起着重要作用，每个图象的类别主要由中心的对象标记，模型需要绝对位置来确定哪个补丁位于中心。</p>
</blockquote>
<p>全局平均池化GAP替换class token</p>
<h2 id="2021-CrossViT-Cross-Attention-Multi-Scale-Vision-Transformer-for-Image-Classification【民间代码】"><a href="#2021-CrossViT-Cross-Attention-Multi-Scale-Vision-Transformer-for-Image-Classification【民间代码】" class="headerlink" title="2021-CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification【民间代码】"></a>2021-CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification【民间代码】</h2><p>【代码】： <a target="_blank" rel="noopener" href="https://github.com/rishikksh20/CrossViT-pytorch">https://github.com/rishikksh20/CrossViT-pytorch</a><br>yangzhi</p>
<h2 id="2021-CAT-Cross-Attention-in-Vision-Transformer【开源】【分类】"><a href="#2021-CAT-Cross-Attention-in-Vision-Transformer【开源】【分类】" class="headerlink" title="2021-CAT: Cross Attention in Vision Transformer【开源】【分类】"></a>2021-CAT: Cross Attention in Vision Transformer【开源】【分类】</h2><p>【代码】： </p>
<h2 id="2021-MSG-Transformer：Exchanging-Local-Spatial-Information-by-Manipulating-Messenger-Tokens【开源】【分类】【结果不好】"><a href="#2021-MSG-Transformer：Exchanging-Local-Spatial-Information-by-Manipulating-Messenger-Tokens【开源】【分类】【结果不好】" class="headerlink" title="2021-MSG-Transformer：Exchanging Local Spatial Information by Manipulating Messenger Tokens【开源】【分类】【结果不好】"></a>2021-MSG-Transformer：Exchanging Local Spatial Information by Manipulating Messenger Tokens【开源】【分类】【结果不好】</h2><p>【代码】： <a target="_blank" rel="noopener" href="https://github.com/hustvl/MSG-Transformer">https://github.com/hustvl/MSG-Transformer</a></p>
<p><strong>摘要：</strong><br>Transformers 提供了一种设计用于视觉识别的神经网络的新方法。 与卷积网络相比，Transformers 享有<strong>在每个阶段引用全局特征的能力</strong>，但注意力模块带来更高的计算开销，阻碍了 Transformer 处理高分辨率视觉数据的应用。 本文旨在缓解效率和灵活性之间的冲突，为此我们为每个区域提出了一个专门的token作为信使（MSG）。 因此，通过<strong>操纵这些 MSG tokens，人们可以跨区域灵活地交换视觉信息</strong>，并降低计算复杂度。 然后，我们将 MSG token集成到名为 MSG-Transformer 的多尺度架构中。 在标准图像分类和目标检测中，MSG-Transformer 实现了具有竞争力的性能，并且加速了 GPU 和 CPU 上的推理。</p>
<h2 id="2021-Swin-Transformer【已开源】【分类】"><a href="#2021-Swin-Transformer【已开源】【分类】" class="headerlink" title="2021-Swin Transformer【已开源】【分类】"></a>2021-Swin Transformer【已开源】【分类】</h2><p>【代码】：<br><a target="_blank" rel="noopener" href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a><br><a target="_blank" rel="noopener" href="https://github.com/berniwal/swin-transformer-pytorch">https://github.com/berniwal/swin-transformer-pytorch</a></p>
<h2 id="2021-Swin-Unet【已开源】【分割】【2021CVPR】"><a href="#2021-Swin-Unet【已开源】【分割】【2021CVPR】" class="headerlink" title="2021-Swin-Unet【已开源】【分割】【2021CVPR】"></a>2021-Swin-Unet【已开源】【分割】【2021CVPR】</h2><p>【代码】： <a target="_blank" rel="noopener" href="https://github.com/HuCaoFighting/Swin-Unet">https://github.com/HuCaoFighting/Swin-Unet</a></p>
<p><img src="https://img-blog.csdnimg.cn/20210715201235141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-UCTransNet-Rethinking-the-Skip-Connections-in-U-Net-from-a-Channel-wise-Perspective-with-Transformer【已开源】【分割】"><a href="#2021-UCTransNet-Rethinking-the-Skip-Connections-in-U-Net-from-a-Channel-wise-Perspective-with-Transformer【已开源】【分割】" class="headerlink" title="2021-UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer【已开源】【分割】"></a>2021-UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer【已开源】【分割】</h2><p>【代码】：<a target="_blank" rel="noopener" href="https://github.com/McGregorWwww/UCTransNet">https://github.com/McGregorWwww/UCTransNet</a></p>
<h2 id="2021-CoTNet-Contextual-Transformer-Networks-for-Visual-Recognition【已开源】【分类】"><a href="#2021-CoTNet-Contextual-Transformer-Networks-for-Visual-Recognition【已开源】【分类】" class="headerlink" title="2021- CoTNet-Contextual Transformer Networks for Visual Recognition【已开源】【分类】"></a>2021- CoTNet-Contextual Transformer Networks for Visual Recognition【已开源】【分类】</h2><p>【代码】：<a target="_blank" rel="noopener" href="https://github.com/JDAI-CV/CoTNet">https://github.com/JDAI-CV/CoTNet</a></p>
<p><del>(代码没太看懂)</del> </p>
<p>本文认为，Transformer的自注意力仅在空域进行信息交互，依赖于输入自身相关性通过独立的方式学习所得，而忽略了近邻间丰富的上下文信息。<br><img src="https://img-blog.csdnimg.cn/465450ac77ed4f428f0de8ed76433800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b696d5f224344f83962e5c40d04e3617.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/5e75b9874b104317941e17de7cf1580c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/dc99ecd85c1e4754a83ef17e0ca33a26.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/8f222dc07041466b9fe0422a4ac4560b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-MaskFormer-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation【已开源】【分割】"><a href="#2021-MaskFormer-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation【已开源】【分割】" class="headerlink" title="2021- MaskFormer- Per-Pixel Classification is Not All You Need for Semantic Segmentation【已开源】【分割】"></a>2021- MaskFormer- Per-Pixel Classification is Not All You Need for Semantic Segmentation【已开源】【分割】</h2><p>【代码】： <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/MaskFormer">https://github.com/facebookresearch/MaskFormer</a><br><img src="https://img-blog.csdnimg.cn/f9754efef4104ee39fa533b6364228b7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/d1a1fbfc70f94991afc34ac291fdc415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/ff26fa3e30c440cf9ab4b0488a1ac1fe.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>MaskFormer的结构如上图所示，主要可以分为三个部分：</p>
<p>1）pixel-level module：用来提取每个像素embedding（灰色背景部分）</p>
<p>2）transformer module：用来计算N个segment的embedding（绿色背景部分）</p>
<p>3）segmentation module：根据上面的per-pixel embedding和per-segment embedding，生成预测结果。（蓝色背景部分）</p>
<p><strong>3.3.1. Pixel-level module</strong><br>在Pixel-level module中，首先用backbone对图片的H和W进行压缩，通道维度进行提升，提取视觉特征，这一部分和正常CNN提取特征类似。然后用一个pixel Decoder去将长宽重新转换为H和W。</p>
<p><strong>3.3.2. Transformer module</strong><br>Transformer module的结构就是标准的Transformer Decoder的结构，根据视觉特征和N个可学习的query来计算输出。</p>
<p><strong>3.3.3. Segmentation module</strong><br>Segmentation  module就是一个FC的结构，后面接上softmax激活函数。它的输出就是segment的概率估计，因此根据这个概率估计和GroundTruth做分类的损失。</p>
<p>对于mask的预测，作者将per-segment embedding通过一个两层的MLP转换成了N个mask embedding。接着，作者将mask embedding和per-pixel embedding进行了点乘，后面接上了sigmoid激活函数，来获得最后mask的预测结果。 </p>
<p><strong>本文亮点总结</strong></p>
<blockquote>
<p>1.作者在这篇论文中提出，其实mask分类是非常通用的，完全可以用mask分类来统一语义分类和实例分类的范式。因此，作者提出了MaskFormer，能够将现有的per-pixel分类的模型转换为mask分类的模型。</p>
<p>2.作者通过实验表明，一个简单的mask分类模型可以优于SOTA的per-pixel分类模型，特别是在存在大量类别的情况下。本文提出的MaskFormer在全景分割任务上也保持着很强的竞争力，最重要的不需要改变模型架构、损失或训练过程。</p>
</blockquote>
<h2 id="2021-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers【已开源】【分割】"><a href="#2021-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers【已开源】【分割】" class="headerlink" title="2021- SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers【已开源】【分割】"></a>2021- SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers【已开源】【分割】</h2><p>【代码】：<a target="_blank" rel="noopener" href="https://github.com/NVlabs/SegFormer">https://github.com/NVlabs/SegFormer</a></p>
<p><img src="https://img-blog.csdnimg.cn/a217a973e0bf4c459c70fc22faab7541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>感觉没啥创新</p>
<ul>
<li>去掉固定形状的位置编码，用3*3卷积作为位置信息补充</li>
<li>编码器部分每层都输出特征图传入后面（层次化）</li>
<li>Overlapped Patch Merging，卷积核大于步长实现</li>
<li>Effiicient Self-Attention，由于计算复杂度和序列长度的平方成正比，所以通过reshape和Linear操作将长度由N=H*W变为N/R，具体操作如下<br><img src="https://img-blog.csdnimg.cn/6cc02129eecc4c339b2690adb7994633.png" alt="在这里插入图片描述"></li>
</ul>
<h2 id="2021-VOLO-Vision-Outlooker-for-Visual-Recognition【已开源】【分类】"><a href="#2021-VOLO-Vision-Outlooker-for-Visual-Recognition【已开源】【分类】" class="headerlink" title="2021-VOLO: Vision Outlooker for Visual Recognition【已开源】【分类】"></a>2021-VOLO: Vision Outlooker for Visual Recognition【已开源】【分类】</h2><p>【代码】：<a target="_blank" rel="noopener" href="https://github.com/sail-sg/volo">https://github.com/sail-sg/volo</a></p>
<p>和token-to-token同一个团队</p>
<h2 id="2021-HaloNet：Scaling-Local-Self-Attention-for-Parameter-Efficient-Visual-Backbones【未开源】【分类】"><a href="#2021-HaloNet：Scaling-Local-Self-Attention-for-Parameter-Efficient-Visual-Backbones【未开源】【分类】" class="headerlink" title="2021- HaloNet：Scaling Local Self-Attention for Parameter Efficient Visual Backbones【未开源】【分类】"></a>2021- HaloNet：Scaling Local Self-Attention for Parameter Efficient Visual Backbones【未开源】【分类】</h2><p>【代码】： </p>
<h2 id="2021-CoAtNet-Marrying-Convolution-and-Attention-for-All-Data-Sizes【未开源】【分类】"><a href="#2021-CoAtNet-Marrying-Convolution-and-Attention-for-All-Data-Sizes【未开源】【分类】" class="headerlink" title="2021- CoAtNet: Marrying Convolution and Attention for All Data Sizes【未开源】【分类】"></a>2021- CoAtNet: Marrying Convolution and Attention for All Data Sizes【未开源】【分类】</h2><p>【代码】：<br>【参考博客】：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247561682&amp;idx=1&amp;sn=5562d0dde1abbc49312864f16c927e81&amp;chksm=ec1d1c2bdb6a953dd763b7e1b8b89fdc5b11cd7ff327880e71ef43b80df88d9c232c113421dd&amp;scene=178&amp;cur_album_id=1685054606675902466#rd">极客平台</a><br>CNN具有归纳偏置（inductive bias），较强的泛化能力，Transformer具有更强的学习能力。本篇文章试图将CNN和Transformer进行结合，引入CNN那种对局部信息的感知，通过这种inductive bias，使得模型在CV任务上具有更好的性能。<br>Depthwise Convolution 的表达式可以写成：<br><img src="https://img-blog.csdnimg.cn/ce4bc96a699b49c1bf8d35f96d369c4b.png" alt="在这里插入图片描述"><br>式中xi和yi表示第 i个位置的输入和输出，Li表示第 i个位置的邻域，比如3×3的邻域。<br>Self-Attention的计算主要分为三步，第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；第二步是使用一个softmax函数对这些权重进行归一化；最后将权重和相应的键值value进行加权求和得到最后的结果。<br><img src="https://img-blog.csdnimg.cn/4cb414bc0de541d7b35b0e4bd422dab4.png" alt="在这里插入图片描述"></p>
<p>Self-attention 的表达式可以写成：<br><img src="https://img-blog.csdnimg.cn/2f6e6434c70840c796c164d58bb49fea.png" alt="在这里插入图片描述"><br>他们各自的特点是：<br><img src="https://img-blog.csdnimg.cn/12a29b0ce9e84e0d8c85489ede1271c5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为了将Conv和Self-Attention的优点结合，可以将静态的全局全局和和自适应注意力矩阵相加，因此就可以表示呈下面的公式：<br><img src="https://img-blog.csdnimg.cn/d09db6f3ae5140669ce32b2612eca4a2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>本文采用yiPre，先softmax，再求和</p>
<h2 id="2021-CSWin-Transformer-A-General-Vision-Transformer-Backbone-with-Cross-Shaped-Windows【已开源】【分类】"><a href="#2021-CSWin-Transformer-A-General-Vision-Transformer-Backbone-with-Cross-Shaped-Windows【已开源】【分类】" class="headerlink" title="2021- CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows【已开源】【分类】"></a>2021- CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows【已开源】【分类】</h2><p>【代码】： <a target="_blank" rel="noopener" href="https://github.com/microsoft/CSWin-Transformer">https://github.com/microsoft/CSWin-Transformer</a><br><img src="https://img-blog.csdnimg.cn/23d7b2d8572b4edfa3fec3cf07270614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b68592f6eccc4f788dc8aa743cddb84e.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/5e67c008a146498fbbeb92b7ab9a644a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为什么每个stage的sw是不一样的？</p>
<p>因为SWin每个stage之后都进行了降采样，所以越到后面的stage，特征的H和W会越来越小。作者在前面的stage中采用了较小的sw，后面的stage中采用较大的sw。</p>
<p>个人觉得这么做主要有两方面的原因：</p>
<p>1）第一，前期H和W较大，如果sw也比较大，那就会导致计算量很大，极端情况下，sw=H或者sw=W，那就相当于直接进行了全局的SA，导致计算量爆炸。因此作者在前期H、W较大的时候，用较小的sw来限制计算量；后期等H、W比较小的时候，在用比较大的sw来扩大感受野。</p>
<p>2）第二，前面的Stage是用来获取局部的attention（这里指宽度较小的十字），后面的stage用获取全局的attention。（但是个人感觉如果这么做的话，就和作者提的Motivation有点冲突了，因为其他ViT结构的由局部慢慢变成全局注意力，如果前面的stage用较小的sw，那么前期也只能捕获局部注意力（十字形的局部），所以就跟其他结构一样是从局部到全局的一个过程。</p>
<h2 id="2021-Transformer-in-Transformer【已开源】【分类】"><a href="#2021-Transformer-in-Transformer【已开源】【分类】" class="headerlink" title="2021- Transformer in Transformer【已开源】【分类】"></a>2021- Transformer in Transformer【已开源】【分类】</h2><p>【代码】：<a target="_blank" rel="noopener" href="https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch">https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch</a><br><img src="https://img-blog.csdnimg.cn/a7feabd26c1f47ee84d517d0159dcab6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-PVT-Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions【已开源】【分类】"><a href="#2021-PVT-Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions【已开源】【分类】" class="headerlink" title="2021-PVT-Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions【已开源】【分类】"></a>2021-PVT-Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions【已开源】【分类】</h2><p>【代码】：<a target="_blank" rel="noopener" href="https://github.com/whai362/PVT">https://github.com/whai362/PVT</a><br><img src="https://img-blog.csdnimg.cn/e18d2477184b4c308d1d075bdfd08172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/9d6e2dbcc8804876ad887425e6399500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-Twins-Revisiting-the-Design-of-Spatial-Attention-in-Vision-Transformers【已开源】【分类】【美团】"><a href="#2021-Twins-Revisiting-the-Design-of-Spatial-Attention-in-Vision-Transformers【已开源】【分类】【美团】" class="headerlink" title="2021-Twins: Revisiting the Design of Spatial Attention in Vision Transformers【已开源】【分类】【美团】"></a>2021-Twins: Revisiting the Design of Spatial Attention in Vision Transformers【已开源】【分类】【美团】</h2><p>【代码】：<a target="_blank" rel="noopener" href="https://github.com/Meituan-AutoML/Twins">https://github.com/Meituan-AutoML/Twins</a><br>CPVT认为，绝对位置编码在处理不同大小的输入时会遇到困难，且绝对位置编码也会打破平移不变性。本文证明，Swin优于PVT的主要原因就是这点，如果使用适当的位置编码，PVT实际上可以实现比Swin Transformer更好的性能<br>提出了两个Transformer结构（第一个完全没创新，就是这个拼那个，第二个主要就是局部注意力做完之后接一个全局）</p>
<blockquote>
<ol>
<li>Twins-PCPVT：用CPVT的条件位置编码CPE来替换PVT中的绝对位置编码。</li>
<li>Twins-SVT：交替局部和全局注意力</li>
</ol>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/3f364b5eca5c44d0a7b658215e15dc26.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/74a79d2bc1b64b74989e382b4f4708d0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-DPT-Deformable-Patch-based-Transformer-for-Visual-Recognition【已开源】【分类】"><a href="#2021-DPT-Deformable-Patch-based-Transformer-for-Visual-Recognition【已开源】【分类】" class="headerlink" title="2021-DPT: Deformable Patch-based Transformer for Visual Recognition【已开源】【分类】"></a>2021-DPT: Deformable Patch-based Transformer for Visual Recognition【已开源】【分类】</h2><p>【代码】： <a target="_blank" rel="noopener" href="https://github.com/CASIA-IVA-Lab/DPT">https://github.com/CASIA-IVA-Lab/DPT</a></p>
<p>代码缺失部分模块<br><img src="https://img-blog.csdnimg.cn/bac411d679ad4b7dacbcbeace21d3383.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASm95Y2Vfbw==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h2 id="2021-Focal-Self-attention-for-Local-Global-Interactions-in-Vision-Transformers【未开源】【分类】"><a href="#2021-Focal-Self-attention-for-Local-Global-Interactions-in-Vision-Transformers【未开源】【分类】" class="headerlink" title="2021-Focal Self-attention for Local-Global Interactions in Vision Transformers【未开源】【分类】"></a>2021-Focal Self-attention for Local-Global Interactions in Vision Transformers【未开源】【分类】</h2><p>【代码】： </p>
<h2 id="2021-Mobile-Former-Bridging-MobileNet-and-Transformer【未开源】【分类】"><a href="#2021-Mobile-Former-Bridging-MobileNet-and-Transformer【未开源】【分类】" class="headerlink" title="2021-Mobile-Former: Bridging MobileNet and Transformer【未开源】【分类】"></a>2021-Mobile-Former: Bridging MobileNet and Transformer【未开源】【分类】</h2><p>【代码】： </p>
<p>轻量级CNN和Transformer桥接，结构挺复杂新颖</p>
<h2 id="2021-MCTrans：【即将开源】【2021MICCAI】"><a href="#2021-MCTrans：【即将开源】【2021MICCAI】" class="headerlink" title="2021-MCTrans：【即将开源】【2021MICCAI】"></a>2021-MCTrans：【即将开源】【2021MICCAI】</h2><p>【代码】： </p>
<p>MCTrans 可以很容易地插入到类似 UNet 的网络中</p>
<h2 id="2021-TransClaw-U-Net-Claw-U-Net-with-Transformers-for-Medical-Image-Segmentation【未开源】【分割】"><a href="#2021-TransClaw-U-Net-Claw-U-Net-with-Transformers-for-Medical-Image-Segmentation【未开源】【分割】" class="headerlink" title="2021-TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation【未开源】【分割】"></a>2021-TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation【未开源】【分割】</h2><p>将CNN和Transformer结合起来，表现SOTA！性能优于TransUNet、AttUNet等网络，作者单位：华东师范大学, 上海交通大学等<br>由于卷积操作的局限性，往往无法准确获取长期的空间特征。因此，我们提出了一种 TransClaw U-Net 网络结构，它在编码部分结合了卷积操作和Transformer操作。卷积部分用于提取浅层空间特征，以利于上采样后图像分辨率的恢复。 Transformer 部分用于对patch 进行编码，使用self-attention 机制获取序列之间的全局信息。解码部分保留了底部上采样结构，以获得更好的细节分割性能。 Synapse Multi-organ Segmentation Datasets 上的实验结果表明。</p>
<p><img src="https://img-blog.csdnimg.cn/20210716221414746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-TransAttUnet-Multi-level-Attention-guided-U-Net-with-Transformer-for-Medical-Image-Segmentation【未开源】【分割】"><a href="#2021-TransAttUnet-Multi-level-Attention-guided-U-Net-with-Transformer-for-Medical-Image-Segmentation【未开源】【分割】" class="headerlink" title="2021-TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation【未开源】【分割】"></a>2021-TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation【未开源】【分割】</h2><p>基于Transformer的多级注意力引导U-Net<br>表现SOTA！性能优于MCTrans、FANet和ResUNet++等网络，作者单位：哈工大(深圳), 香港中文大学<br>随着深度编码器-解码器架构和大规模注释医学数据集的发展，自动医学图像分割的发展取得了很大进展。由于卷积层的堆叠和连续的采样操作，现有的标准模型不可避免地会遇到特征表示的信息衰退问题，无法完全建模全局上下文特征依赖关系。为了克服上述挑战，本文提出了一种新的基于 Transformer 的医学图像语义分割框架 TransAttUnet，其中联合设计了多级引导注意和多尺度跳过连接，以有效增强传统 U 形的功能性和灵活性。建筑学。受 Transformer 的启发，一个具有 Transformer Self Attention (TSA) 和 Global Spatial Attention (GSA) 的新型自感知注意力 (SAA) 模块被整合到 TransAttUnet 中，以有效地学习编码器特征之间的非局部交互。特别是，我们还在解码器块之间建立了额外的多尺度跳跃连接，以聚合不同的语义尺度上采样特征。通过这种方式，增强了多尺度上下文信息的表示能力以生成判别特征。受益于这些互补的组件，所提出的 TransAttUnet 可以有效缓解信息衰退问题导致的精细细节丢失，提高医学图像分析的诊断灵敏度和分割质量。对不同成像的多个医学图像分割数据集的大量实验表明，我们的方法始终优于最先进的基线。<br><img src="https://img-blog.csdnimg.cn/20210716221547822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-FFVT-Feature-Fusion-Vision-Transformer-for-Fine-Grained-Visual-Categorization【未开源】【分类】"><a href="#2021-FFVT-Feature-Fusion-Vision-Transformer-for-Fine-Grained-Visual-Categorization【未开源】【分类】" class="headerlink" title="2021-FFVT-Feature Fusion Vision Transformer for Fine-Grained Visual Categorization【未开源】【分类】"></a>2021-FFVT-Feature Fusion Vision Transformer for Fine-Grained Visual Categorization【未开源】【分类】</h2><p>用于细粒度视觉分类的特征融合视觉Transformer<br>据作者称，这是第一个探索视觉Transformer在小规模和大规模细粒度视觉分类上性能的研究，并提出一种基于纯Transformer的框架特征融合ViT：FFVT，表现SOTA！性能优于TransFG、NTS-Net等网络，单位：格里菲斯大学<br>处理细粒度视觉分类 (FGVC) 的核心是学习细微但有辨别力的特征。以前的大多数工作都是通过显式选择判别部分或通过基于 CNN 的方法集成注意力机制来实现的。 然而，这些方法增加了计算复杂度，并使模型由包含最多对象的区域主导。最近，视觉转换器（ViT）在一般图像识别任务上实现了 SOTA 性能。自注意力机制将所有patch的信息聚合并加权到分类令牌，使其非常适合 FGVC。尽管如此，深度的分类token更关注全局信息，缺乏 FGVC 必不可少的局部和低级特征。在这项工作中，我们提出了一种新颖的纯基于Transformer的框架特征融合视觉Transformer（FFVT），我们聚合来自每个Transformer层的重要标记来补偿局部、低级和中级信息。我们设计了一个称为相互注意权重选择 (MAWS) 的新型token选择模块，以有效和高效地引导网络在不引入额外参数的情况下选择判别性token。我们在三个基准测试中验证了 FFVT 的有效性，其中 FFVT 实现了最先进的性能。<br><img src="https://img-blog.csdnimg.cn/20210716221802692.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Feature fusion is exploited before the last transformer layer to aggregate the important local, low-level and middle level information from previous layers. This is implemented by replacing the inputs (exclude classification token) of the last transformer layer with the tokens selected by the MAWS Module.<br><strong>Feature Fusion Module</strong><br>MSA在深层更倾向于关注全局信息，所以提出特征融合模块来补偿局部信息。<br>具体操作：将最后一层transformer层的输入替换为前面每层Transformer经过MAWS筛选后的tokens。In this way, the class token in the last transformer layer fully interacts with the low-level, middle level and high-level features from the previous layers, enriching the local information and feature representation capability.<br><img src="https://img-blog.csdnimg.cn/20210716222815133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Our feature fusion strategy is motivated by [12]【Transfg: A transformer architecture for fine-grained recognition】 which only selects the tokens from the penultimate transformer layer, yet our method directly aggregates important information from each layer.</p>
<p><strong>Mutual AttentionWeight Selection Module 相互注意权重选择模块</strong><br>提出一个token selection方法，通过直接利用多头自注意力模块生成的attention scores<br>来选择。<br><img src="https://img-blog.csdnimg.cn/20210716223332481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>其中，aij是token i 和token j关于token i上下文中的注意得分<br>一个简单的策略是选择和分类token有更高注意得分的token，因为分类token包含丰富的类别信息。如果用这种方法的话，可以直接对a0进行排序，然后选择值最大的token。这种方法称为single attention weight selection（SAWS）。但这种方法可能会引入噪声信息因为被选择的token可能聚合了更多来自噪声patches的信息。<br>拿3个patch块的attention得分矩阵来举例：<br><img src="https://img-blog.csdnimg.cn/2021071622395455.png" alt="在这里插入图片描述"><br>Token three is selected as it has the biggest value in the attention score vector for classification token. However, token three aggregates much information from token one (the<br> maximum attention score in a3) thus may introduce noises assuming token one is a noisy token。<br>为了解决这种问题，提出一种mutual attention weight selection module，需要被选择的token和分类token在分类token的上下文和它自己的上下文中都相似。<br>将attention得分矩阵的第一列表示为b0，b0即为分类token和其他token在其他token上下文中的attention得分向量。<br>最终，mutual attention weight（MAW）mai为：<br><img src="https://img-blog.csdnimg.cn/2021071622564865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-TransFG-A-Transformer-Architecture-for-Fine-grained-Recognition【已开源】【分类】"><a href="#2021-TransFG-A-Transformer-Architecture-for-Fine-grained-Recognition【已开源】【分类】" class="headerlink" title="2021-TransFG: A Transformer Architecture for Fine-grained Recognition【已开源】【分类】"></a>2021-TransFG: A Transformer Architecture for Fine-grained Recognition【已开源】【分类】</h2><p>细粒度视觉分类中最重要的问题之一是准确定位解释相似子类之间细微差异的判别区域</p>
<h2 id="2021-LG-Transformer-Local-to-Global-Self-Attention-in-Vision-Transformer【未开源】【分类】"><a href="#2021-LG-Transformer-Local-to-Global-Self-Attention-in-Vision-Transformer【未开源】【分类】" class="headerlink" title="2021-LG-Transformer-Local-to-Global Self-Attention in Vision Transformer【未开源】【分类】"></a>2021-LG-Transformer-Local-to-Global Self-Attention in Vision Transformer【未开源】【分类】</h2><p>Transformer的缺点很明显：计算复杂度和输入的特征大小呈平方的关系。因此直接对整张图片进行Self-Attention是不现实的，所以，最近的一些工作（比如Swin-Transformer[1]）采用了像CNN一样的分层结构，每层施加注意力的范围只在local window上，逐渐扩大Self-Attention的感知范围。<br>作者提出，这样的方式存在一定的缺点，因为在前面几个stage中没有对global的特征进行感知，因此，作者就提出了一种多分支的Transformer设计结构，使得Transformer在每个stage中都进同时进行全局和局部的信息感知。通过引入多分支结构，使得模型在分类任务和语义分割任务上都取得了一定的性能提升。<br>希望在前面的stage中也能加入全局的信息感知，所以作者引入了多分支结构，有的分支用来捕获全局的注意力（先降采样），有的分支用来捕获局部注意力，相当于是在Transformer中采用了一个类似Inception的结构<br><img src="https://img-blog.csdnimg.cn/img_convert/60d270a3a12e2f2b6cbd564deca1c198.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/img_convert/4f61c338e01cb3b8b4340bea7b6ab6da.png" alt="在这里插入图片描述"></p>
<h2 id="2021-CCTrans-Simplifying-and-Improving-Crowd-Counting-with-Transformer"><a href="#2021-CCTrans-Simplifying-and-Improving-Crowd-Counting-with-Transformer" class="headerlink" title="2021-CCTrans: Simplifying and Improving Crowd Counting with Transformer"></a>2021-CCTrans: Simplifying and Improving Crowd Counting with Transformer</h2><ul>
<li><p>单位：哈尔滨工业大学（深圳）、美团</p>
</li>
<li><p>Arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.14483">https://arxiv.org/abs/2109.14483</a></p>
</li>
<li><p>Github: 即将开源</p>
</li>
</ul>
<p>人群计数</p>
<p>&lt;hr style=” border:solid; width:100px; height:1px;” color=#000000 size=1”&gt;</p>
<h2 id="2021-Tokens-to-Token-ViT：Training-Vision-Transformers-from-Scratch-on-ImageNet【已开源】【分类】"><a href="#2021-Tokens-to-Token-ViT：Training-Vision-Transformers-from-Scratch-on-ImageNet【已开源】【分类】" class="headerlink" title="2021-Tokens-to-Token ViT：Training Vision Transformers from Scratch on ImageNet【已开源】【分类】"></a>2021-Tokens-to-Token ViT：Training Vision Transformers from Scratch on ImageNet【已开源】【分类】</h2><p>预训练图片大小： 384<br>【代码】： <a target="_blank" rel="noopener" href="https://github.com/yitu-opensource/T2T-ViT">https://github.com/yitu-opensource/T2T-ViT</a><br>本文认为中等大小数据集上，Transformer性能低于CNN的原因有两点：</p>
<ol>
<li>ViT处理图片的方式不够好，无法建模一张图片的局部信息</li>
<li>ViT的自注意力极值的Backbone不如CNN设计的更好</li>
</ol>
<p>采用类似CNN中卷积划窗的方式，将相邻的tokens局部聚合起来，有助于建模局部特征。另外还设计了一种deep narrow（个人理解是 深+窄 的网络结构）结构，减少了运算量，并获得性能上的提升。<br>​​​​​​​​<br><img src="https://img-blog.csdnimg.cn/20210709094733464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>本文先分析了Resnet50，Vision Transformer，T2T Transformer的特征可视化。其中绿框标注的是浅层特征，如边缘，线条。红框标注全白全黑的是一些零值或过大值，对最终预测没有贡献</p>
<p>我们先从熟悉的CNN看起，在比较浅的层中，网络学习到的更多是结构信息，比如对这只小狗边缘的刻画。随着层数加深，通道数变深，特征也越来越抽象</p>
<p>再来看ViT，他每层都能很好的建模全局信息，即使是很深的层当中，也没有所谓非常抽象的东西。但它对结构信息捕捉的很少，（个人认为是没有类似CNN卷积核划窗的操作，导致对局部信息捕捉不够？）ViT的表现不尽相同，浅层特征和中层特征的冗余度很高，很多channels的可视化结果十分相似，缺乏边、角、纹理等信息）</p>
<p>Token to Token 结构</p>
<p><img src="https://img-blog.csdnimg.cn/20210709094746875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Vision Transformer是将二维图片展平成一维向量（也叫token），然后送入到Transoformer结构里。而T2T为了捕捉局部信息，它将所有的token通过reshape操作，恢复成二维，然后利用一个unfold一个划窗操作，属于一个窗口的tokens，会连接成一个更长的token，然后送入到Transformer中。</p>
<p>这样会逐渐减少token的数量，但随之而来token的长度会增加很多（因为多个tokens连接在一个token），因此后续模型也降低了维度数目，以平衡计算量。</p>
<p>经过比较作者得到两个结论：</p>
<ol>
<li>使用Deep-narrow架构，并减少embedding dimension更适合视觉Transformer，可以增加特征的丰富程度，同时减少计算量</li>
<li>SE模块的channel attention结构也可以提升ViT性能，但结果不如1</li>
</ol>
<p><strong>关于Unfold操作</strong><br>Unfold操作其实就是卷积中用到的img2col方法，将一个卷积窗口的向量，重排成一个列向量。<br><img src="https://img-blog.csdnimg.cn/5133320a402a47739bcac1b65537f58f.png" alt="在这里插入图片描述"></p>
<p>下面是一段测试代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_input = np.array([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                       [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]],</span><br><span class="line">                     [[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">                      [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">                      [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]],</span><br><span class="line">                     [[<span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>],</span><br><span class="line">                      [<span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>],</span><br><span class="line">                      [<span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>]],</span><br><span class="line">                     [[<span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>],</span><br><span class="line">                      [<span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>],</span><br><span class="line">                      [<span class="number">34</span>, <span class="number">35</span>, <span class="number">36</span>]]]]).astype(np.float32)</span><br><span class="line"></span><br><span class="line">torch_input = torch.Tensor(np_input)</span><br><span class="line"></span><br><span class="line">unfold = torch.nn.Unfold(kernel_size=<span class="number">2</span>, padding=<span class="number">0</span>, stride=<span class="number">1</span>)</span><br><span class="line">unfolded = unfold(torch_input)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(unfolded)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">         [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">         [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">13.</span>, <span class="number">14.</span>],</span><br><span class="line">         [<span class="number">11.</span>, <span class="number">12.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">16.</span>, <span class="number">17.</span>],</span><br><span class="line">         [<span class="number">14.</span>, <span class="number">15.</span>, <span class="number">17.</span>, <span class="number">18.</span>],</span><br><span class="line">         [<span class="number">19.</span>, <span class="number">20.</span>, <span class="number">22.</span>, <span class="number">23.</span>],</span><br><span class="line">         [<span class="number">20.</span>, <span class="number">21.</span>, <span class="number">23.</span>, <span class="number">24.</span>],</span><br><span class="line">         [<span class="number">22.</span>, <span class="number">23.</span>, <span class="number">25.</span>, <span class="number">26.</span>],</span><br><span class="line">         [<span class="number">23.</span>, <span class="number">24.</span>, <span class="number">26.</span>, <span class="number">27.</span>],</span><br><span class="line">         [<span class="number">28.</span>, <span class="number">29.</span>, <span class="number">31.</span>, <span class="number">32.</span>],</span><br><span class="line">         [<span class="number">29.</span>, <span class="number">30.</span>, <span class="number">32.</span>, <span class="number">33.</span>],</span><br><span class="line">         [<span class="number">31.</span>, <span class="number">32.</span>, <span class="number">34.</span>, <span class="number">35.</span>],</span><br><span class="line">         [<span class="number">32.</span>, <span class="number">33.</span>, <span class="number">35.</span>, <span class="number">36.</span>]]])</span><br></pre></td></tr></table></figure>
<p>这是对应的示意图<br><img src="https://img-blog.csdnimg.cn/20210709094809380.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>T2T ViT</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">T2T_ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, tokens_type=<span class="string">&#x27;performer&#x27;</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path_rate=<span class="number">0.</span>, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.num_features = self.embed_dim = embed_dim  <span class="comment"># num_features for consistency with other models</span></span><br><span class="line"></span><br><span class="line">        self.tokens_to_token = T2T_module(</span><br><span class="line">            img_size=img_size, tokens_type=tokens_type, in_chans=in_chans, embed_dim=embed_dim)</span><br><span class="line">        num_patches = self.tokens_to_token.num_patches</span><br><span class="line"></span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line">        self.pos_embed = nn.Parameter(data=get_sinusoid_encoding(n_position=num_patches + <span class="number">1</span>, d_hid=embed_dim),</span><br><span class="line">                                      requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)]  <span class="comment"># stochastic depth decay rule</span></span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block(</span><br><span class="line">                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line">        self.norm = norm_layer(embed_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Classifier head</span></span><br><span class="line">        self.head = nn.Linear(embed_dim, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        trunc_normal_(self.cls_token, std=<span class="number">.02</span>)</span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line">    ...忽略一些其他的方法</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        B = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = self.tokens_to_token(x)</span><br><span class="line"></span><br><span class="line">        cls_tokens = self.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">        x = self.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x = blk(x)</span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.forward_features(x)</span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>整个架构是将输入数据通过T2Tmodule，然后设立一个分类的token(cls_tokens)，将其concat到x中，并加入position embedding（这里是用一个可学习参数作为位置编码）。处理好后，输入到一个个叠起来的Transformer Block，最后取第一个token（也就是cls_tokens)，输入到分类层，得到最终结果。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://houyuanyuan99.github.io">HouYuanyuan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://houyuanyuan.github.io/2021/07/10/011-Vision-Transformer/">http://houyuanyuan.github.io/2021/07/10/011-Vision-Transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://houyuanyuan.github.io" target="_blank">ʕ̯•͡˔•̯᷅ʔ</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%B0%83%E7%A0%94/">调研</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://img-blog.csdnimg.cn/a7feabd26c1f47ee84d517d0159dcab6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/04/009-Python-Pytorch%E7%AC%94%E8%AE%B0/"><img class="prev-cover" src="/img/post_cover/python.jpg" onerror="onerror=null;src='/img/bg/18.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Python&amp;&amp;Pytorch笔记</div></div></a></div><div class="next-post pull-right"><a href="/2021/03/09/008-%E7%BB%8F%E9%AA%8C%E6%95%99%E8%AE%AD/"><img class="next-cover" src="/img/post_cover/bug.jpeg" onerror="onerror=null;src='/img/bg/18.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">经验教训&amp;&amp;遇到的Bug们</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/08/17/010-%E5%A4%9A%E6%A8%A1%E6%80%81%E7%AE%80%E5%8D%95%E8%B0%83%E7%A0%94/" title="多模态简单调研"><img class="cover" src="/img/post_cover/%E5%A4%9A%E6%A8%A1%E6%80%81.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-17</div><div class="title">多模态简单调研</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-CoaT%EF%BC%9ACo-Scale-Conv-Attentional-ImageTransformers%E3%80%90%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">1.</span> <span class="toc-text">2021-CoaT：Co-Scale Conv-Attentional ImageTransformers【开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-CPVT%EF%BC%9AConditional-Positional-Encodings-for-Vision-Transformer%E3%80%90%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91%E3%80%90%E7%BE%8E%E5%9B%A2%E3%80%91"><span class="toc-number">2.</span> <span class="toc-text">2021-CPVT：Conditional Positional Encodings for Vision Transformer【开源】【分类】【美团】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-CrossViT-Cross-Attention-Multi-Scale-Vision-Transformer-for-Image-Classification%E3%80%90%E6%B0%91%E9%97%B4%E4%BB%A3%E7%A0%81%E3%80%91"><span class="toc-number">3.</span> <span class="toc-text">2021-CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification【民间代码】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-CAT-Cross-Attention-in-Vision-Transformer%E3%80%90%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">4.</span> <span class="toc-text">2021-CAT: Cross Attention in Vision Transformer【开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-MSG-Transformer%EF%BC%9AExchanging-Local-Spatial-Information-by-Manipulating-Messenger-Tokens%E3%80%90%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91%E3%80%90%E7%BB%93%E6%9E%9C%E4%B8%8D%E5%A5%BD%E3%80%91"><span class="toc-number">5.</span> <span class="toc-text">2021-MSG-Transformer：Exchanging Local Spatial Information by Manipulating Messenger Tokens【开源】【分类】【结果不好】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-Swin-Transformer%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">6.</span> <span class="toc-text">2021-Swin Transformer【已开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-Swin-Unet%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E5%89%B2%E3%80%91%E3%80%902021CVPR%E3%80%91"><span class="toc-number">7.</span> <span class="toc-text">2021-Swin-Unet【已开源】【分割】【2021CVPR】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-UCTransNet-Rethinking-the-Skip-Connections-in-U-Net-from-a-Channel-wise-Perspective-with-Transformer%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E5%89%B2%E3%80%91"><span class="toc-number">8.</span> <span class="toc-text">2021-UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer【已开源】【分割】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-CoTNet-Contextual-Transformer-Networks-for-Visual-Recognition%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">9.</span> <span class="toc-text">2021- CoTNet-Contextual Transformer Networks for Visual Recognition【已开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-MaskFormer-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E5%89%B2%E3%80%91"><span class="toc-number">10.</span> <span class="toc-text">2021- MaskFormer- Per-Pixel Classification is Not All You Need for Semantic Segmentation【已开源】【分割】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E5%89%B2%E3%80%91"><span class="toc-number">11.</span> <span class="toc-text">2021- SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers【已开源】【分割】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-VOLO-Vision-Outlooker-for-Visual-Recognition%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">12.</span> <span class="toc-text">2021-VOLO: Vision Outlooker for Visual Recognition【已开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-HaloNet%EF%BC%9AScaling-Local-Self-Attention-for-Parameter-Efficient-Visual-Backbones%E3%80%90%E6%9C%AA%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">13.</span> <span class="toc-text">2021- HaloNet：Scaling Local Self-Attention for Parameter Efficient Visual Backbones【未开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-CoAtNet-Marrying-Convolution-and-Attention-for-All-Data-Sizes%E3%80%90%E6%9C%AA%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">14.</span> <span class="toc-text">2021- CoAtNet: Marrying Convolution and Attention for All Data Sizes【未开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-CSWin-Transformer-A-General-Vision-Transformer-Backbone-with-Cross-Shaped-Windows%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">15.</span> <span class="toc-text">2021- CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows【已开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-Transformer-in-Transformer%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">16.</span> <span class="toc-text">2021- Transformer in Transformer【已开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-PVT-Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">17.</span> <span class="toc-text">2021-PVT-Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions【已开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-Twins-Revisiting-the-Design-of-Spatial-Attention-in-Vision-Transformers%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91%E3%80%90%E7%BE%8E%E5%9B%A2%E3%80%91"><span class="toc-number">18.</span> <span class="toc-text">2021-Twins: Revisiting the Design of Spatial Attention in Vision Transformers【已开源】【分类】【美团】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-DPT-Deformable-Patch-based-Transformer-for-Visual-Recognition%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">19.</span> <span class="toc-text">2021-DPT: Deformable Patch-based Transformer for Visual Recognition【已开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-Focal-Self-attention-for-Local-Global-Interactions-in-Vision-Transformers%E3%80%90%E6%9C%AA%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">20.</span> <span class="toc-text">2021-Focal Self-attention for Local-Global Interactions in Vision Transformers【未开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-Mobile-Former-Bridging-MobileNet-and-Transformer%E3%80%90%E6%9C%AA%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">21.</span> <span class="toc-text">2021-Mobile-Former: Bridging MobileNet and Transformer【未开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-MCTrans%EF%BC%9A%E3%80%90%E5%8D%B3%E5%B0%86%E5%BC%80%E6%BA%90%E3%80%91%E3%80%902021MICCAI%E3%80%91"><span class="toc-number">22.</span> <span class="toc-text">2021-MCTrans：【即将开源】【2021MICCAI】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-TransClaw-U-Net-Claw-U-Net-with-Transformers-for-Medical-Image-Segmentation%E3%80%90%E6%9C%AA%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E5%89%B2%E3%80%91"><span class="toc-number">23.</span> <span class="toc-text">2021-TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation【未开源】【分割】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-TransAttUnet-Multi-level-Attention-guided-U-Net-with-Transformer-for-Medical-Image-Segmentation%E3%80%90%E6%9C%AA%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E5%89%B2%E3%80%91"><span class="toc-number">24.</span> <span class="toc-text">2021-TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation【未开源】【分割】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-FFVT-Feature-Fusion-Vision-Transformer-for-Fine-Grained-Visual-Categorization%E3%80%90%E6%9C%AA%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">25.</span> <span class="toc-text">2021-FFVT-Feature Fusion Vision Transformer for Fine-Grained Visual Categorization【未开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-TransFG-A-Transformer-Architecture-for-Fine-grained-Recognition%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">26.</span> <span class="toc-text">2021-TransFG: A Transformer Architecture for Fine-grained Recognition【已开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-LG-Transformer-Local-to-Global-Self-Attention-in-Vision-Transformer%E3%80%90%E6%9C%AA%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">27.</span> <span class="toc-text">2021-LG-Transformer-Local-to-Global Self-Attention in Vision Transformer【未开源】【分类】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-CCTrans-Simplifying-and-Improving-Crowd-Counting-with-Transformer"><span class="toc-number">28.</span> <span class="toc-text">2021-CCTrans: Simplifying and Improving Crowd Counting with Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-Tokens-to-Token-ViT%EF%BC%9ATraining-Vision-Transformers-from-Scratch-on-ImageNet%E3%80%90%E5%B7%B2%E5%BC%80%E6%BA%90%E3%80%91%E3%80%90%E5%88%86%E7%B1%BB%E3%80%91"><span class="toc-number">29.</span> <span class="toc-text">2021-Tokens-to-Token ViT：Training Vision Transformers from Scratch on ImageNet【已开源】【分类】</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://img-blog.csdnimg.cn/a7feabd26c1f47ee84d517d0159dcab6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70')"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By HouYuanyuan</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>