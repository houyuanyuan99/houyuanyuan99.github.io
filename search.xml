<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>位运算</title>
    <url>/2022/07/02/001-%E4%BD%8D%E8%BF%90%E7%AE%97/</url>
    <content><![CDATA[<!-- ![这里输入图片描述](/力扣.png) -->
<!-- ![](/img/leetcode.png) -->
<!-- <img src="/2022/07/02/001-%E4%BD%8D%E8%BF%90%E7%AE%97/leetcode.png" class="" title="This is an test image"> -->
<h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><h3 id="191-位1的个数"><a href="#191-位1的个数" class="headerlink" title="191.位1的个数"></a>191.位1的个数</h3><p>题目链接：<a href="https://leetcode.cn/problems/number-of-1-bits/">https://leetcode.cn/problems/number-of-1-bits/</a></p>
<p>若 $n = 2^x$  且 x为自然数（即 n 为 2 的幂），则恒有 $ n \&amp;(n - 1) == 0 $ ，这是因为：<br>$n$ 二进制最高位为 1，其余所有位为0；<br>$n−1$ 二进制最高位为 0，其余所有位为 1；<br><!-- ![](/1.png) --><br><img src="/2022/07/02/001-%E4%BD%8D%E8%BF%90%E7%AE%97/1.png" class title="This is an test image"></p>
<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><h3 id="and运算-amp"><a href="#and运算-amp" class="headerlink" title="and运算&amp;"></a>and运算&amp;</h3><p>  and运算通常用于二进制的取位操作，例如一个数 and 1的结果就是取二进制的最末位。这可以用来判断一个整数的奇偶，二进制的最末位为0表示该数为偶数，最末位为1表示该数为奇数。<br>相同位的两个数字都为1，则为1；若有一个不为1，则为0。</p>
<h3 id="or运算"><a href="#or运算" class="headerlink" title="or运算 |"></a>or运算 |</h3><p>or运算通常用于二进制特定位上的无条件赋值，例如一个数or 1的结果就是把二进制最末位强行变成1。如果需要把二进制最末位变成0，对这个数or 1之后再减一就可以了，其实际意义就是把这个数强行变成最接近的偶数。<br>相同位只要一个为1即为1。</p>
<h3 id="xor运算"><a href="#xor运算" class="headerlink" title="xor运算 ^"></a>xor运算 ^</h3><p>异或的符号是^。按位异或运算, 对等长二进制模式按位或二进制数的每一位执行逻辑按位异或操作. 操作的结果是如果某位不同则该位为1, 否则该位为0.<br>xor运算的逆运算是它本身，也就是说两次异或同一个数最后结果不变，即(a xor b) xor b = a。xor运算可以用于简单的加密。<br>相同位不同则为1，相同则为0。</p>
<h3 id="not运算"><a href="#not运算" class="headerlink" title="not运算 ~"></a>not运算 ~</h3><p>not运算的定义是把内存中的0和1全部取反。使用not运算时要格外小心，你需要注意整数类型有没有符号。如果not的对象是无符号整数（不能表示负数），那么得到的值就是它与该类型上界的差，因为无符号类型的数是用00到$FFFF依次表示的。</p>
<h3 id="左移-lt-lt"><a href="#左移-lt-lt" class="headerlink" title="左移 &lt;&lt;"></a>左移 &lt;&lt;</h3><p>a &lt;&lt; b就表示把a转为二进制后左移b位（在后面添b个0）。例如100的二进制为1100100，而110010000转成十进制是400，那么100 shl 2 = 400。可以看出，a shl b的值实际上就是a乘以2的b次方，因为在二进制数后添一个0就相当于该数乘以2。<br>通常认为a &lt;&lt; 1比a * 2更快，因为前者是更底层一些的操作。因此程序中乘以2的操作请尽量用左移一位来代替。<br>定义一些常量可能会用到&lt;&lt;运算。你可以方便地用1 &lt;&lt; 16 - 1来表示65535。很多算法和数据结构要求数据规模必须是2的幂，此时可以用&lt;&lt;来定义Max_N等常量。</p>
<h3 id="带符号右移-gt-gt"><a href="#带符号右移-gt-gt" class="headerlink" title="带符号右移&gt;&gt;"></a>带符号右移&gt;&gt;</h3><p>和&lt;&lt;相似，a &gt;&gt; b表示二进制右移b位（去掉末b位,最左边用0填充），相当于a除以2的b次方（取整）。和上面一样的例子，那么400 &gt;&gt;2 = 100。我们也经常用&gt;&gt;1来代替div 2，比如二分查找、堆的插入操作等等。想办法用&gt;&gt;代替除法运算可以使程序效率大大提高。最大公约数的二进制算法用除以2操作来代替慢得出奇的mod运算，效率可以提高60%。</p>
<h3 id="一些技巧"><a href="#一些技巧" class="headerlink" title="一些技巧"></a>一些技巧</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"># 整数集合set位运算</span><br><span class="line"># 整数集合做标志时，比如回溯时的visited标志数组</span><br><span class="line">vstd 访问 i ：vstd | (<span class="number">1</span> &lt;&lt; i)</span><br><span class="line">vstd 离开 i ：vstd &amp; ~(<span class="number">1</span> &lt;&lt; i)</span><br><span class="line">vstd 不包含 i : <span class="keyword">not</span> vstd &amp; (<span class="number">1</span> &lt;&lt; i)</span><br><span class="line">并集 ：A | B</span><br><span class="line">交集 ：A &amp; B</span><br><span class="line">全集 ：(<span class="number">1</span> &lt;&lt; n) - <span class="number">1</span></span><br><span class="line">补集 ：((<span class="number">1</span> &lt;&lt; n) - <span class="number">1</span>) ^ A</span><br><span class="line">子集 ：(A &amp; B) == B</span><br><span class="line">判断是否是 <span class="number">2</span> 的幂 ：A &amp; (A - <span class="number">1</span>) == <span class="number">0</span></span><br><span class="line">最低位的 <span class="number">1</span> 变为 <span class="number">0</span> ：n &amp;= (n - <span class="number">1</span>)</span><br><span class="line">最低位的 <span class="number">1</span>：A &amp; (-A)，最低位的 <span class="number">1</span> 一般记为 <span class="built_in">lowbit</span>(A)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>程序设计</category>
      </categories>
      <tags>
        <tag>力扣</tag>
      </tags>
  </entry>
  <entry>
    <title>力扣刷题记录</title>
    <url>/2022/07/05/002-%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><h3 id="关于力扣上的代码如何在本地编译运行"><a href="#关于力扣上的代码如何在本地编译运行" class="headerlink" title="关于力扣上的代码如何在本地编译运行"></a>关于力扣上的代码如何在本地编译运行</h3><p>定义个main函数，构造个输入用例，然后定义一个solution变量，调用函数就可以了<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minCostClimbingStairs</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; cost)</span> </span>&#123;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(cost.size())</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = cost[<span class="number">0</span>];</span><br><span class="line">        dp[<span class="number">1</span>] = cost[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">2</span>; i &lt; cost.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            dp[i] = <span class="built_in">min</span>(dp[i - <span class="number">1</span>], dp[i - <span class="number">2</span>]) + cost[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">min</span>(dp[cost.<span class="built_in">size</span>() - <span class="number">1</span>], dp[cost.<span class="built_in">size</span>() - <span class="number">2</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> a[] = &#123;<span class="number">1</span>, <span class="number">100</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">cost</span><span class="params">(a, a + <span class="keyword">sizeof</span>(a) / <span class="keyword">sizeof</span>(<span class="type">int</span>))</span></span>;</span><br><span class="line">    Solution solution;</span><br><span class="line">    cout &lt;&lt; solution.<span class="built_in">minCostClimbingStairs</span>(cost) &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="第300场周赛补题-蔚来"><a href="#第300场周赛补题-蔚来" class="headerlink" title="第300场周赛补题-蔚来"></a>第300场周赛补题-蔚来</h2><h3 id="2325-解密消息（简单）"><a href="#2325-解密消息（简单）" class="headerlink" title="2325. 解密消息（简单）"></a>2325. 解密消息（简单）</h3><p>给你字符串 key 和 message ，分别表示一个加密密钥和一段加密消息。解密 message 的步骤如下：</p>
<p>使用 key 中 26 个英文小写字母第一次出现的顺序作为替换表中的字母 顺序 。<br>将替换表与普通英文字母表对齐，形成对照表。<br>按照对照表 替换 message 中的每个字母。<br>空格 ‘ ‘ 保持不变。<br>例如，key = “happy boy”（实际的加密密钥会包含字母表中每个字母 至少一次），据此，可以得到部分对照表（’h’ -&gt; ‘a’、’a’ -&gt; ‘b’、’p’ -&gt; ‘c’、’y’ -&gt; ‘d’、’b’ -&gt; ‘e’、’o’ -&gt; ‘f’）。</p>
<p>题目链接：<a href="https://leetcode.cn/problems/decode-the-message">https://leetcode.cn/problems/decode-the-message</a></p>
<p><strong>示例</strong></p>
<img src="/2022/07/05/002-%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/2325.jpg" class title="This is an test image">
<blockquote>
<p>输入：key = “the quick brown fox jumps over the lazy dog”, message = “vkbs bs t suepuv”<br>输出：”this is a secret”<br>解释：对照表如上图所示。<br>提取 “the quick brown fox jumps over the lazy dog” 中每个字母的首次出现可以得到替换表。</p>
</blockquote>
<p>对map之类的容器生疏了，比赛的时候写的太复杂了，这里复习一遍用法加强印象<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">decodeMessage</span><span class="params">(string key, string message)</span> </span>&#123;</span><br><span class="line">        map&lt;<span class="type">char</span>,<span class="type">char</span>&gt; mp;</span><br><span class="line">        mp[<span class="string">&#x27; &#x27;</span>] = <span class="string">&#x27; &#x27;</span>;</span><br><span class="line">        <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;key.<span class="built_in">size</span>();++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(!mp[key[i]])</span><br><span class="line">            &#123;</span><br><span class="line">                mp[key[i]] = <span class="string">&#x27;a&#x27;</span> + cnt;</span><br><span class="line">                cnt++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;message.<span class="built_in">size</span>();++i)</span><br><span class="line">        &#123;</span><br><span class="line">            message[i] = mp[message[i]];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> message;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h3 id="2326-螺旋矩阵-IV（中等）"><a href="#2326-螺旋矩阵-IV（中等）" class="headerlink" title="2326. 螺旋矩阵 IV（中等）"></a>2326. 螺旋矩阵 IV（中等）</h3><p>给你两个整数：m 和 n ，表示矩阵的维数。</p>
<p>另给你一个整数链表的头节点 head 。</p>
<p>请你生成一个大小为 m x n 的螺旋矩阵，矩阵包含链表中的所有整数。链表中的整数从矩阵 左上角 开始、顺时针 按 螺旋 顺序填充。如果还存在剩余的空格，则用 -1 填充。</p>
<p>返回生成的矩阵。</p>
<p>题目链接：<a href="https://leetcode.cn/problems/spiral-matrix-iv">https://leetcode.cn/problems/spiral-matrix-iv</a></p>
<p><strong>示例</strong><br><img src="/2022/07/05/002-%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/2326.jpg" class title="This is an test image"></p>
<blockquote>
<p>输入：m = 3, n = 5, head = [3,0,2,6,8,1,7,9,4,2,5,5,0]<br>输出：[[3,0,2,6,8],[5,0,-1,-1,1],[5,2,4,9,7]]<br>解释：上图展示了链表中的整数在矩阵中是如何排布的。<br>注意，矩阵中剩下的空格用 -1 填充。</p>
</blockquote>
<p><strong>思路</strong><br>在矩阵内四个方向依次轮转，当越界或者当前的格子已经填过数时，即转换方向</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode() : val(0), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> dx[<span class="number">4</span>] = &#123;<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> dy[<span class="number">4</span>] = &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>&#125;;</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">spiralMatrix</span>(<span class="type">int</span> m, <span class="type">int</span> n, ListNode* head) &#123;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">ans</span>(m,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n));</span><br><span class="line">        <span class="type">int</span> cur = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> i=<span class="number">0</span>,j=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> x=<span class="number">0</span>;x&lt;m;x++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> y=<span class="number">0</span>;y&lt;n;++y)</span><br><span class="line">            &#123;</span><br><span class="line">                ans[x][y] = <span class="number">-1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        ans[<span class="number">0</span>][<span class="number">0</span>] = head-&gt;val;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">        <span class="keyword">while</span>(head)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(j+dx[cur]&lt;<span class="number">0</span>||j+dx[cur]&gt;=n||i+dy[cur]&lt;<span class="number">0</span>||i+dy[cur]&gt;=m||ans[i+dy[cur]][j+dx[cur]]!=<span class="number">-1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                cur = (cur+<span class="number">1</span>)%<span class="number">4</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            i+=dy[cur];j+=dx[cur];</span><br><span class="line">            ans[i][j] = head-&gt;val;</span><br><span class="line">            head = head-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="2327-知道秘密的人数（中等）"><a href="#2327-知道秘密的人数（中等）" class="headerlink" title="2327. 知道秘密的人数（中等）"></a>2327. 知道秘密的人数（中等）</h3><p>在第 1 天，有一个人发现了一个秘密。</p>
<p>给你一个整数 delay ，表示每个人会在发现秘密后的 delay 天之后，每天 给一个新的人 分享 秘密。同时给你一个整数 forget ，表示每个人在发现秘密 forget 天之后会 忘记 这个秘密。一个人 不能 在忘记秘密那一天及之后的日子里分享秘密。</p>
<p>给你一个整数 n ，请你返回在第 n 天结束时，知道秘密的人数。由于答案可能会很大，请你将结果对 10e + 7 取余 后返回。</p>
<p>题目链接：<a href="https://leetcode.cn/problems/number-of-people-aware-of-a-secret">https://leetcode.cn/problems/number-of-people-aware-of-a-secret</a></p>
<p><strong>示例</strong></p>
<blockquote>
<p>输入：n = 6, delay = 2, forget = 4<br>输出：5<br>解释：<br>第 1 天：假设第一个人叫 A 。（一个人知道秘密）<br>第 2 天：A 是唯一一个知道秘密的人。（一个人知道秘密）<br>第 3 天：A 把秘密分享给 B 。（两个人知道秘密）<br>第 4 天：A 把秘密分享给一个新的人 C 。（三个人知道秘密）<br>第 5 天：A 忘记了秘密，B 把秘密分享给一个新的人 D 。（三个人知道秘密）<br>第 6 天：B 把秘密分享给 E，C 把秘密分享给 F 。（五个人知道秘密）</p>
</blockquote>
<p><strong>思路</strong><br>递推，动态规划</p>
<p><strong>解法1</strong><br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> mod =<span class="number">1e9</span>+<span class="number">7</span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">peopleAwareOfSecret</span><span class="params">(<span class="type">int</span> n, <span class="type">int</span> delay, <span class="type">int</span> forget)</span> </span>&#123;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(n+<span class="number">2</span>,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n+<span class="number">2</span>)); <span class="comment">//考虑用long long</span></span><br><span class="line">        dp[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span>; <span class="comment">//dp[i][j]表示在第i天知道秘密天数为第j天的人</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=delay;j&lt;forget;++j)</span><br><span class="line">            &#123;</span><br><span class="line">                dp[i][<span class="number">1</span>] = (dp[i<span class="number">-1</span>][j]+dp[i][<span class="number">1</span>])%mod;</span><br><span class="line">                <span class="comment">//第i天新知道消息的人消息来源来自前一天知道消息delay天~forget天的人，求和</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">2</span>;j&lt;=forget;++j)</span><br><span class="line">            &#123;</span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>];</span><br><span class="line">                <span class="comment">//第i天知道消息j天（j&gt;=2）的人是第i-1天知道消息j-1天的人</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=forget;++i)</span><br><span class="line">        &#123;</span><br><span class="line">            ans = (ans+dp[n][i])%mod;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br><strong>解法2</strong><br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> mod =<span class="number">1e9</span>+<span class="number">7</span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">peopleAwareOfSecret</span><span class="params">(<span class="type">int</span> n, <span class="type">int</span> delay, <span class="type">int</span> forget)</span> </span>&#123;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(n+<span class="number">2</span>)</span></span>;<span class="comment">//表示第i天新增的知道消息的人数</span></span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=i-forget+<span class="number">1</span>;j&lt;=i-delay;++j)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(j&gt;<span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    dp[i] += dp[j];</span><br><span class="line">                    dp[i] = dp[i]%mod;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=n;i&gt;=n-forget+<span class="number">1</span>;--i)</span><br><span class="line">        &#123;</span><br><span class="line">            ans = (ans+dp[i])%mod;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h3 id="2328-网格图中递增路径的数目（困难）"><a href="#2328-网格图中递增路径的数目（困难）" class="headerlink" title="2328. 网格图中递增路径的数目（困难）"></a>2328. 网格图中递增路径的数目（困难）</h3><p>给你一个 m x n 的整数网格图 grid ，你可以从一个格子移动到 4 个方向相邻的任意一个格子。</p>
<p>请你返回在网格图中从 任意 格子出发，达到 任意 格子，且路径中的数字是 严格递增 的路径数目。由于答案可能会很大，请将结果对 10e9 + 7 取余 后返回。</p>
<p>如果两条路径中访问过的格子不是完全相同的，那么它们视为两条不同的路径</p>
<p>题目链接：<a href="https://leetcode.cn/problems/number-of-increasing-paths-in-a-grid">https://leetcode.cn/problems/number-of-increasing-paths-in-a-grid</a></p>
<p><strong>示例</strong><br><img src="https://assets.leetcode.com/uploads/2022/05/10/griddrawio-4.png" alt></p>
<blockquote>
<p>输入：grid = [[1,1],[3,4]]<br>输出：8<br>解释：严格递增路径包括：</p>
<pre><code>- 长度为 1 的路径：[1]，[1]，[3]，[4] 。
- 长度为 2 的路径：[1 -&gt; 3]，[1 -&gt; 4]，[3 -&gt; 4] 。
- 长度为 3 的路径：[1 -&gt; 3 -&gt; 4] 。
路径数目为 4 + 3 + 1 = 8 。
</code></pre></blockquote>
<p><strong>思路</strong><br>记忆化搜索<br>记录 f(i, j) 表示以 (i, j) 为结尾的不同路径有几条。计算 f(i,j) 时，检查 (i, j) 上下左右四个格子，若相邻格子里的数比当前格子里的数小，则可以转移给 f(i, j)。当然最后别忘了计算从 (i, j)开始，然后直接在 (i, j) 结束的路径，也就是 f(i, j) 还要再加 1。</p>
<p>用记忆化搜索即可在 $ O(nm) $ 的复杂度内完成。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> mod =<span class="number">1e9</span>+<span class="number">7</span>;</span><br><span class="line">    vector&lt;vector&lt;<span class="type">long</span> <span class="type">long</span>&gt;&gt; f;</span><br><span class="line">    <span class="comment">// long long f[1010][1010];</span></span><br><span class="line">    <span class="type">int</span> n,m;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> dx[<span class="number">4</span>] = &#123;<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> dy[<span class="number">4</span>] = &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>&#125;;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> y,vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; grid)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        f[x][y] = <span class="number">1</span>; <span class="comment">//长度为1的路径</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">4</span>;++i) <span class="comment">//检查相邻格子</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> tx = x+dx[i];<span class="type">int</span> ty = y+dy[i];</span><br><span class="line">            <span class="keyword">if</span>(tx&lt;n&amp;&amp;tx&gt;=<span class="number">0</span>&amp;&amp;ty&lt;m&amp;&amp;ty&gt;=<span class="number">0</span>&amp;&amp;grid[x][y]&gt;grid[tx][ty])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(!f[tx][ty]) <span class="built_in">dfs</span>(tx,ty,grid);</span><br><span class="line">                f[x][y] = (f[x][y]+f[tx][ty])%mod;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">countPaths</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; grid)</span> </span>&#123;</span><br><span class="line">        n = grid.<span class="built_in">size</span>();</span><br><span class="line">        m = grid[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        f.<span class="built_in">resize</span>(n, <span class="built_in">vector</span>&lt;<span class="type">long</span> <span class="type">long</span>&gt;(m));</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;m;++j)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(f[i][j]==<span class="number">0</span>) <span class="built_in">dfs</span>(i,j,grid);</span><br><span class="line">                ans = (ans+f[i][j])%mod;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> (<span class="type">int</span>)ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="第132场周赛"><a href="#第132场周赛" class="headerlink" title="第132场周赛"></a>第132场周赛</h2><h3 id="5024-除数博弈（简单）"><a href="#5024-除数博弈（简单）" class="headerlink" title="5024. 除数博弈（简单）"></a>5024. 除数博弈（简单）</h3><p>爱丽丝和鲍勃一起玩游戏，他们轮流行动。爱丽丝先手开局。<br>最初，黑板上有一个数字 N 。在每个玩家的回合，玩家需要执行以下操作：<br>选出任一 x，满足 0 &lt; x &lt; N 且 N % x == 0 。<br>用 N - x 替换黑板上的数字 N 。<br>如果玩家无法执行这些操作，就会输掉游戏。<br>只有在爱丽丝在游戏中取得胜利时才返回 True，否则返回 false。假设两个玩家都以最佳状态参与游戏。<br><strong>思路:</strong> 草稿纸手算各种情况，这种简单博弈论的题首先就判断奇偶的情况就对了<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">   <span class="function"><span class="type">bool</span> <span class="title">divisorGame</span><span class="params">(<span class="type">int</span> N)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">if</span>(N % <span class="number">2</span> == <span class="number">0</span>)  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">   <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"> 	&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h3 id="5030-节点与其祖先之间的最大差值"><a href="#5030-节点与其祖先之间的最大差值" class="headerlink" title="5030.节点与其祖先之间的最大差值"></a>5030.节点与其祖先之间的最大差值</h3><p>给定二叉树的根节点 root，找出存在于不同节点 A 和 B 之间的最大值 V，其中 V = |A.val - B.val|，且 A 是 B 的祖先。<br>（如果 A 的任何子节点之一为 B，或者 A 的任何子节点是 B 的祖先，那么我们认为 A 是 B 的祖先）</p>
<p><strong>思路:</strong> 从根节点出发开始搜索，一方面比较当前节点和最大值/最小值的绝对值，一方面更新最大最小值，因为是顺着一条条支路走下去的，所以结果一定是祖先和其后代的差值<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> * int val;</span></span><br><span class="line"><span class="comment"> * TreeNode *left;</span></span><br><span class="line"><span class="comment"> * TreeNode *right;</span></span><br><span class="line"><span class="comment"> * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(TreeNode* root,<span class="type">int</span> x,<span class="type">int</span> y,<span class="type">int</span> &amp;ans)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span>(!root) <span class="keyword">return</span> ;</span><br><span class="line">ans = <span class="built_in">max</span>(ans,<span class="built_in">abs</span>(x-root-&gt;val));</span><br><span class="line">ans = <span class="built_in">max</span>(ans,<span class="built_in">abs</span>(y-root-&gt;val));</span><br><span class="line">x = <span class="built_in">max</span>(x,root-&gt;val);</span><br><span class="line">y = <span class="built_in">min</span>(y,root-&gt;val);</span><br><span class="line"><span class="built_in">dfs</span>(root-&gt;left,x,y,ans);</span><br><span class="line"><span class="built_in">dfs</span>(root-&gt;right,x,y,ans);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">maxAncestorDiff</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line"><span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> x,y;x = y = root-&gt;val;</span><br><span class="line"><span class="built_in">dfs</span>(root,x,y,ans);</span><br><span class="line"><span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h3 id="5025-最长等差数列"><a href="#5025-最长等差数列" class="headerlink" title="5025. 最长等差数列"></a>5025. 最长等差数列</h3><p>给定一个整数数组 A，返回 A 中最长等差子序列的长度。<br><strong>思路：</strong> 设置一个dp[i][j],表示以i结尾的差值为j的最长等差序列的个数，但要注意，这里不能用二维数组，因为j有可能为负数，所以最省力的做法就是unordered_map</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">longestArithSeqLength</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; A)</span> </span>&#123;</span><br><span class="line">unordered_map&lt;<span class="type">int</span>,<span class="type">int</span>&gt; dp[<span class="number">2005</span>];</span><br><span class="line"><span class="type">int</span> len = A.<span class="built_in">size</span>();</span><br><span class="line"><span class="type">int</span> ans = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; len; ++i)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> j = i+<span class="number">1</span>; j &lt; len; ++j)</span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> cha = A[j]-A[i];</span><br><span class="line">dp[j][cha]=<span class="built_in">max</span>(dp[j][cha],dp[i][cha]+<span class="number">1</span>);</span><br><span class="line">ans = <span class="built_in">max</span>(ans,dp[j][cha]+<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>程序设计</category>
      </categories>
      <tags>
        <tag>力扣</tag>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title>Matlab的安装及破译</title>
    <url>/2019/04/01/004-Matlab%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E7%A0%B4%E8%AF%91/</url>
    <content><![CDATA[<p>写在前面：和数字图像处理一段不解之缘的开始（ps：已换电脑，还没重装matlab）</p>
<p><strong>第一步</strong><br>放上下载链接<br>链接: <a href="https://pan.baidu.com/s/1nxw14nOfQeMiYYLmkNO-cw">https://pan.baidu.com/s/1nxw14nOfQeMiYYLmkNO-cw</a><br>提取码:dw1h<br>（版本为2017a，文件内容过大，所以分成了两部分，建议下载之后新建一个文件夹，将两部分解压至一处）</p>
<p><strong>第二步</strong><br>点击setup.exe，等待稍许，出现如下界面，选择使用文件安装密钥<br><img src="/2019/04/01/004-Matlab%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E7%A0%B4%E8%AF%91/1.jpg" class title="This is an test image"></p>
<p><strong>第三步</strong><br>输入文件安装密钥，可在破译文件中找到<br><img src="/2019/04/01/004-Matlab%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E7%A0%B4%E8%AF%91/%E5%AE%89%E8%A3%852.png" class title="This is an test image"><br><strong>第四步</strong><br>选择合适的位置安装位置，文件较大，不建议使用系统默认盘</p>
<p><strong>第五步</strong><br>勾选全部产品，等待安装</p>
<img src="/2019/04/01/004-Matlab%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E7%A0%B4%E8%AF%91/%E5%AE%89%E8%A3%853.png" class title="This is an test image">
<p>很久很久之后……<br><strong>第六步</strong><br>（如果没有将将部分解压到同一个文件夹，这个过程会暂停，此时仍需要把part2中的两个文件复制到1所在文件夹）<br>安装完成后，点击bin目录下的matlab.exe，进行软件激活<br><strong>第七步</strong><br>选择“在不使用Internet的情况下激活”，与此同时，将破译文件下的license_standalone.lic 复制到 \MATLAB\R2017a\licenses\，netapi32.dll 复制到 \MATLAB\R2017a\bin\win64\<br><strong>第八步</strong><br>输入许可证文件的完整路径，即为刚刚复制的license_standalone.lic</p>
<p>大功告成</p>
]]></content>
      <categories>
        <category>经验记录</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title>数据集汇总和介绍</title>
    <url>/2022/07/04/003-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B1%87%E6%80%BB%E5%92%8C%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/mou_it/article/details/82225505">https://blog.csdn.net/mou_it/article/details/82225505</a></p>
<p><a href="https://www.jianshu.com/p/16b2e32d9edf">https://www.jianshu.com/p/16b2e32d9edf</a></p>
<p>目标检测开源数据集： <a href="https://mp.weixin.qq.com/s/M-RnUKaHeZg5-yJU2WDuhA">https://mp.weixin.qq.com/s/M-RnUKaHeZg5-yJU2WDuhA</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习和部分代码</title>
    <url>/2019/10/18/007-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%83%A8%E5%88%86%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<h3 id="线性回归最小二乘法代码"><a href="#线性回归最小二乘法代码" class="headerlink" title="线性回归最小二乘法代码"></a>线性回归最小二乘法代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">points = np.genfromtxt(<span class="string">&#x27;data.txt&#x27;</span>,delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">x = points[:,<span class="number">0</span>]<span class="comment">#第一列数据</span></span><br><span class="line">y = points[:,<span class="number">1</span>]</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">w,b,points</span>):</span><br><span class="line">    total_cost = <span class="number">0</span></span><br><span class="line">    M = <span class="built_in">len</span>(points)</span><br><span class="line">    <span class="comment">#逐点计算实际数据与模型数据的平方差，求平均值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        x = points[i,<span class="number">0</span>]</span><br><span class="line">        y = points[i,<span class="number">1</span>]</span><br><span class="line">        total_cost += (y - w*x - b)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_cost/M</span><br><span class="line"><span class="comment">#定义求均值函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">average</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">    num = <span class="built_in">len</span>(data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">        <span class="built_in">sum</span>+=data[i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>/num</span><br><span class="line"><span class="comment">#定义核心拟函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">points</span>):</span><br><span class="line">    M = <span class="built_in">len</span>(points)</span><br><span class="line">    x_bar = average(points[:,<span class="number">0</span>])</span><br><span class="line">    sum_yx = <span class="number">0</span></span><br><span class="line">    sum_x2 = <span class="number">0</span></span><br><span class="line">    sum_delta = <span class="number">0</span></span><br><span class="line">    <span class="comment">#根据公式计算w</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        x = points[i,<span class="number">0</span>]</span><br><span class="line">        y = points[i,<span class="number">1</span>]</span><br><span class="line">        sum_yx += y * (x - x_bar)</span><br><span class="line">        sum_x2 += x**<span class="number">2</span></span><br><span class="line">    w = sum_yx / (sum_x2 - M * (x_bar**<span class="number">2</span>))</span><br><span class="line">    <span class="comment">#根据公式计算b</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        x = points[i,<span class="number">0</span>]</span><br><span class="line">        y = points[i,<span class="number">1</span>]</span><br><span class="line">        sum_delta += y - w*x</span><br><span class="line">    b = sum_delta / M</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w,b</span><br><span class="line"></span><br><span class="line">w,b = fit(points)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w is :&#x27;</span>,w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b is :&#x27;</span>,b)</span><br><span class="line">cost = compute_cost(w,b,points)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;cost is :&#x27;</span>,cost)</span><br><span class="line"></span><br><span class="line">pred_y = w*x + b</span><br><span class="line">plt.plot(x, pred_y, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="sklearn线性回归"><a href="#sklearn线性回归" class="headerlink" title="sklearn线性回归"></a>sklearn线性回归</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">boston = load_boston()</span><br><span class="line">x = boston.data[:,<span class="number">5</span>:<span class="number">6</span>]<span class="comment">#取数据第五列为x的值</span></span><br><span class="line">y = boston.target</span><br><span class="line"><span class="comment">#建立线性回归模型</span></span><br><span class="line">regre = LinearRegression()</span><br><span class="line">model2 = regre.fit(x,y)</span><br><span class="line">a = regre.coef_ <span class="comment">#直线的斜率</span></span><br><span class="line">b = regre.intercept_ <span class="comment">#直线的截距</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y = %d * x + (%d)&quot;</span>%(a,b))</span><br><span class="line">pre = model2.predict(x)</span><br><span class="line"><span class="comment">#真实的点</span></span><br><span class="line">plt.scatter(x,y,color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"><span class="comment">#拟合的直线</span></span><br><span class="line">plt.plot(x,pre)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure>
<h3 id="线性回归标准方程法"><a href="#线性回归标准方程法" class="headerlink" title="线性回归标准方程法"></a>线性回归标准方程法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_wb</span>(<span class="params">X,y</span>):</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(X.T*X)!=<span class="number">0</span>:</span><br><span class="line">        wb = ((X.T.dot(X).I).dot(X.T)).dot(y)</span><br><span class="line">        <span class="keyword">return</span> wb</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">x,wb</span>):</span><br><span class="line">    <span class="keyword">return</span> x.T.dot(wb)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getdata</span>():</span><br><span class="line">    x = []</span><br><span class="line">    y = []</span><br><span class="line">    file = <span class="built_in">open</span>(<span class="string">&quot;ex0.txt&quot;</span>,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file.readlines():</span><br><span class="line">        temp = line.strip().split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">        x.append([<span class="built_in">float</span>(temp[<span class="number">0</span>]),<span class="built_in">float</span>(temp[<span class="number">1</span>])])</span><br><span class="line">        y.append(<span class="built_in">float</span>(temp[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> (np.mat(x),np.mat(y).T)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw</span>(<span class="params">x,y,wb</span>):</span><br><span class="line">    a = np.linspace(<span class="number">0</span>,np.<span class="built_in">max</span>(x)) <span class="comment">#等差数列函数</span></span><br><span class="line">    b = wb[<span class="number">0</span>] + a*wb[<span class="number">1</span>]</span><br><span class="line">    plot(x,y,<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    plot(a,b)</span><br><span class="line">    show()</span><br><span class="line"></span><br><span class="line">X,y = getdata()</span><br><span class="line">wb = train_wb(X,y)</span><br><span class="line">draw(X[:,<span class="number">1</span>],y,wb.tolist())</span><br></pre></td></tr></table></figure>
<h3 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadDataSet</span>(<span class="params">fileName</span>):</span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = []</span><br><span class="line">        curLine = line.strip().split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(curLine)):</span><br><span class="line">            lineArr.append(<span class="built_in">float</span>(curLine[i]))</span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">    dataArr = np.array(dataMat)</span><br><span class="line">    <span class="keyword">return</span> dataArr</span><br><span class="line"></span><br><span class="line">dataArr = loadDataSet(<span class="string">&#x27;diabetes_train.txt&#x27;</span>)</span><br><span class="line">testArr = loadDataSet(<span class="string">&#x27;diabetes_train.txt&#x27;</span>)</span><br><span class="line">labels = dataArr[:,:<span class="number">1</span>]</span><br><span class="line">testlabel = testArr[:,:<span class="number">1</span>]</span><br><span class="line">dataArr = dataArr[:,<span class="number">1</span>:]</span><br><span class="line">testArr = testArr[:,<span class="number">1</span>:]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kNN</span>(<span class="params">dataArr,labels,testArr,k</span>):</span><br><span class="line">    testAns = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(testArr)):</span><br><span class="line">        distSquareMat = (dataArr - testArr[i]) ** <span class="number">2</span>  <span class="comment"># 计算差值的平方</span></span><br><span class="line">        distSquareSums = distSquareMat.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">        distance = distSquareSums**<span class="number">0.5</span></span><br><span class="line">        sortedIndices = distance.argsort()<span class="comment">#排序，得到排序后的下标</span></span><br><span class="line">        indices = sortedIndices[:k]</span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> indices:</span><br><span class="line">            labe = labels[j]</span><br><span class="line">            <span class="keyword">if</span> labe == <span class="number">1</span> :</span><br><span class="line">                num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> num &gt; (k-num):</span><br><span class="line">            testAns.append([<span class="number">1.0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            testAns.append([<span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">return</span> testAns</span><br><span class="line"></span><br><span class="line">ans = kNN(dataArr,labels,testArr,<span class="number">5</span>)</span><br><span class="line">length = testlabel.shape[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;糖尿病测试集原结果如下：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(testlabel.tolist())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;5近邻算法求得测试集结果如下：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(ans)</span><br><span class="line">lostSum = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>,<span class="number">2</span>):</span><br><span class="line">    ans = kNN(dataArr,labels,testArr,i)</span><br><span class="line">    lost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">        lost += <span class="built_in">abs</span>( <span class="built_in">int</span>( ans[j]-testlabel[j] ) )</span><br><span class="line">    <span class="comment">#lostSum.append(lost)</span></span><br><span class="line">    lostSum[i] = lost</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当k取值为如下奇数时，预测错误个数分别为：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(lostSum)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>研究生期间</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Codeforces刷题记录</title>
    <url>/2019/04/17/005-Codeforces%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h2 id="Codeforces-552-Div-3"><a href="#Codeforces-552-Div-3" class="headerlink" title="Codeforces 552 (Div. 3)"></a>Codeforces 552 (Div. 3)</h2><h3 id="A-Restoring-Three-Numbers"><a href="#A-Restoring-Three-Numbers" class="headerlink" title="A. Restoring Three Numbers"></a>A. Restoring Three Numbers</h3><p><strong>题意</strong>：给你四个数，a+b，a+c，b+c，a+b+c，（abc都为正数，可以相等）让你找出满足条件的a、b、c的值，排名不分先后<br><strong>思路</strong>：找出四个数中最大的数，减去其他三个就ok了了<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> a[<span class="number">6</span>];</span><br><span class="line">    <span class="type">int</span> maxn = <span class="number">0</span>,flag = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= <span class="number">4</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class="line">        <span class="keyword">if</span>(a[i] &gt; maxn)</span><br><span class="line">        &#123;</span><br><span class="line">            maxn = a[i];flag = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i&lt;=<span class="number">4</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(i!=flag)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>,maxn-a[i]);</span><br><span class="line">            cnt++;</span><br><span class="line">            <span class="keyword">if</span>(cnt&lt;=<span class="number">2</span>) <span class="built_in">printf</span>(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="B-Make-Them-Equal"><a href="#B-Make-Them-Equal" class="headerlink" title="B. Make Them Equal"></a>B. Make Them Equal</h3><p><strong>题意</strong>：给定一个序列，让其中的每一个数通过+x，-x，或不变三种方式变成同一个数，求最小的非负数x，不存在输出-1<br><strong>思路</strong>：数列中超过三种数肯定是错的，所以先用set求不同数的个数<br>如果只有一种数，每个数不变就好，答案为0；<br>两种，有两个情况，他们有中位数，直接中间的数减去小的数，没有就只能是两个数的差值；<br>三种，只有等差数列才满足条件<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">105</span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n,tem;</span><br><span class="line">    <span class="type">int</span> num[<span class="number">5</span>];</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>,&amp;n);</span><br><span class="line">    <span class="type">int</span> a = <span class="number">0</span>,b = <span class="number">0</span>,c = <span class="number">0</span>;</span><br><span class="line">    set&lt;<span class="type">int</span>&gt; s;</span><br><span class="line">    set&lt;<span class="type">int</span>&gt; ::iterator it;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>,&amp;tem);</span><br><span class="line">        s.<span class="built_in">insert</span>(tem);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> x = s.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span>(x&gt;<span class="number">3</span>) <span class="built_in">printf</span>(<span class="string">&quot;-1\n&quot;</span>);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(x == <span class="number">1</span>) <span class="built_in">printf</span>(<span class="string">&quot;0\n&quot;</span>);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> i = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(it=s.<span class="built_in">begin</span>();it!=s.<span class="built_in">end</span>();it++)</span><br><span class="line">        &#123;</span><br><span class="line">            num[i++] = *it;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(x == <span class="number">2</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> sum = num[<span class="number">1</span>]+num[<span class="number">2</span>];</span><br><span class="line">            <span class="keyword">if</span>(sum%<span class="number">2</span>==<span class="number">0</span>) <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,sum/<span class="number">2</span>-num[<span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,num[<span class="number">2</span>]-num[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> sum = num[<span class="number">1</span>]+num[<span class="number">3</span>];</span><br><span class="line">            <span class="keyword">if</span>(sum == num[<span class="number">2</span>]*<span class="number">2</span>) <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>,num[<span class="number">2</span>]-num[<span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">&quot;-1\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="C-Gourmet-Cat"><a href="#C-Gourmet-Cat" class="headerlink" title="C. Gourmet Cat"></a>C. Gourmet Cat</h3><p>这一题当时思索了一下，先跳过去写D，然后再重新思考，交题的时候刚好断网，换卡开热点开写<br><strong>题意</strong>：一只猫从星期一到星期天每天固定只吃1 2 3 1 3 2 1类食物，三类食物分别有a、b、c数量，主人想出去旅游，从星期几开始无所谓，问这只贵气的猫最多能撑多久<br><strong>思路</strong>：数据比较大啊……($1≤a,b,c≤7⋅10^8$)不能纯暴力。可以看出每周1、2、3类食物消耗的量分别是3、2、2，于是先用a、b、c除以3、2、2，看看哪种食物最不能撑<br>然后分别多留出一周，分三种情况暴力处理从一周每一天开始的情况，求出坚持最大天数才使三类存粮最慢消耗为负数<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">200005</span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n,a,b,c;</span><br><span class="line">    <span class="type">int</span> s[<span class="number">8</span>][<span class="number">15</span>]=&#123;</span><br><span class="line">    &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>&#125;,</span><br><span class="line">    &#123;<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>&#125;,</span><br><span class="line">    &#123;<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>&#125;,</span><br><span class="line">    &#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;,</span><br><span class="line">    &#123;<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>&#125;,</span><br><span class="line">    &#123;<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>&#125;,</span><br><span class="line">    &#123;<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>&#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d %d %d&quot;</span>,&amp;a,&amp;b,&amp;c);</span><br><span class="line">    <span class="type">int</span> x,y,z;</span><br><span class="line">    x = a/<span class="number">3</span>;</span><br><span class="line">    y = b/<span class="number">2</span>;</span><br><span class="line">    z = c/<span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(x&lt;=y&amp;&amp;x&lt;=z)</span><br><span class="line">    &#123;</span><br><span class="line">        x--;</span><br><span class="line">        ans+=x*<span class="number">7</span>;</span><br><span class="line">        a-=<span class="number">3</span>*x;b-=<span class="number">2</span>*x;c-=<span class="number">2</span>*x;</span><br><span class="line">        <span class="type">int</span> ma=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">7</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">            x=a,y=b,z=c;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;<span class="number">14</span>;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(s[i][j]==<span class="number">1</span>) x--;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(s[i][j]==<span class="number">2</span>) y--;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(s[i][j]==<span class="number">3</span>) z--;</span><br><span class="line">                <span class="keyword">if</span>(y&lt;<span class="number">0</span>||x&lt;<span class="number">0</span>||z&lt;<span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">                cnt++;</span><br><span class="line">            &#125;</span><br><span class="line">            ma=<span class="built_in">max</span>(ma,cnt);</span><br><span class="line">        &#125;</span><br><span class="line">        ans+=ma;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(y&lt;x&amp;&amp;y&lt;z)</span><br><span class="line">    &#123;</span><br><span class="line">        y--;</span><br><span class="line">        ans+=y*<span class="number">7</span>;</span><br><span class="line">        a-=<span class="number">3</span>*y;b-=<span class="number">2</span>*y;c-=<span class="number">2</span>*y;</span><br><span class="line">        <span class="type">int</span> ma=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">7</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">            x=a,y=b,z=c;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;<span class="number">14</span>;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(s[i][j]==<span class="number">1</span>) x--;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(s[i][j]==<span class="number">2</span>) y--;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(s[i][j]==<span class="number">3</span>) z--;</span><br><span class="line">                <span class="keyword">if</span>(y&lt;<span class="number">0</span>||x&lt;<span class="number">0</span>||z&lt;<span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">                cnt++;</span><br><span class="line">            &#125;</span><br><span class="line">            ma=<span class="built_in">max</span>(ma,cnt);</span><br><span class="line">        &#125;</span><br><span class="line">        ans+=ma;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        z--;</span><br><span class="line">        ans+=z*<span class="number">7</span>;</span><br><span class="line">        a-=<span class="number">3</span>*z;b-=<span class="number">2</span>*z;c-=<span class="number">2</span>*z;</span><br><span class="line">        <span class="type">int</span> ma=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">7</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">            x=a,y=b,z=c;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;<span class="number">14</span>;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(s[i][j]==<span class="number">1</span>) x--;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(s[i][j]==<span class="number">2</span>) y--;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(s[i][j]==<span class="number">3</span>) z--;</span><br><span class="line">                <span class="keyword">if</span>(y&lt;<span class="number">0</span>||x&lt;<span class="number">0</span>||z&lt;<span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">                cnt++;</span><br><span class="line">            &#125;</span><br><span class="line">            ma=<span class="built_in">max</span>(ma,cnt);</span><br><span class="line">        &#125;</span><br><span class="line">        ans+=ma;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,ans);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="D-Walking-Robot"><a href="#D-Walking-Robot" class="headerlink" title="D. Walking Robot"></a>D. Walking Robot</h3><p><strong>题意:</strong> 一个机器人要从0走到n，它拥有b节普通电池和a节太阳能蓄力电池，每走一步路需要消耗一节电池，这段路上另外有个数组si表示i-1到i处是否有太阳，为1表示有太阳，有阳光的话可以给不在使用的太阳能蓄力电池充电，求小机器人最多能走几步（n封顶）<br><strong>思路:</strong> 一遍模拟，精髓就在于一定要先用太阳能电池，其他没啥了<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">200005</span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n,a,b,x,y;</span><br><span class="line">    <span class="type">int</span> s[maxn];</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d %d %d&quot;</span>,&amp;n,&amp;a,&amp;b);</span><br><span class="line">    x = a; y = b;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>,&amp;s[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> flag = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(y&lt;b&amp;&amp;x&gt;<span class="number">0</span>&amp;&amp;s[i+<span class="number">1</span>]==<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            y++;x--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(y&gt;<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            y--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(x&gt;<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            x--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            flag = i;<span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(flag == <span class="number">-1</span>) <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,n);</span><br><span class="line">    <span class="keyword">else</span>    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,flag);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>程序设计</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Codeforces</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构与算法</title>
    <url>/2022/07/10/006-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><h3 id="数组理论基础"><a href="#数组理论基础" class="headerlink" title="数组理论基础"></a>数组理论基础</h3><h3 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h3><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><h2 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h2><h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><h2 id="双指针法"><a href="#双指针法" class="headerlink" title="双指针法"></a>双指针法</h2><h2 id="栈与队列"><a href="#栈与队列" class="headerlink" title="栈与队列"></a>栈与队列</h2><h2 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h2><h2 id="回溯算法"><a href="#回溯算法" class="headerlink" title="回溯算法"></a>回溯算法</h2><h2 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h2><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><h3 id="记忆化搜索"><a href="#记忆化搜索" class="headerlink" title="记忆化搜索"></a>记忆化搜索</h3><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><h3 id="广度优先搜索、深度优先搜索"><a href="#广度优先搜索、深度优先搜索" class="headerlink" title="广度优先搜索、深度优先搜索"></a>广度优先搜索、深度优先搜索</h3>]]></content>
      <categories>
        <category>程序设计</category>
      </categories>
      <tags>
        <tag>力扣</tag>
        <tag>CPP</tag>
        <tag>Offer</tag>
      </tags>
  </entry>
  <entry>
    <title>经验教训&amp;&amp;遇到的Bug们</title>
    <url>/2021/03/09/008-%E7%BB%8F%E9%AA%8C%E6%95%99%E8%AE%AD/</url>
    <content><![CDATA[<h2 id="安装mmcv和mmseg"><a href="#安装mmcv和mmseg" class="headerlink" title="安装mmcv和mmseg"></a>安装mmcv和mmseg</h2><h3 id="安装mmcv"><a href="#安装mmcv" class="headerlink" title="安装mmcv"></a>安装mmcv</h3><p>源码地址：<br><a href="https://github.com/open-mmlab/mmcv">https://github.com/open-mmlab/mmcv</a><br>对应表格确认自己环境的pytorch版本和cuda版本<br>按照指定命令安装<br>注意！！！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install mmcv-full==&#123;mmcv_version&#125; -f https://download.openmmlab.com/mmcv/dist/&#123;cu_version&#125;/&#123;torch_version&#125;/index.html</span><br></pre></td></tr></table></figure>
<p>其中的{mmcv_version}要替换为具体的版本号<br>如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install mmcv-full==<span class="number">1.3</span><span class="number">.9</span> -f https://download.openmmlab.com/mmcv/dist/cu111/torch1<span class="number">.9</span><span class="number">.0</span>/index.html</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/e98242f8462a4c6d941f4b697d8ba980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="安装mmseg"><a href="#安装mmseg" class="headerlink" title="安装mmseg"></a>安装mmseg</h3><p>虽然代码中用的是import mmseg<br>但是安装的时候mmseg要写全<br>命令为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install mmsegmentation</span><br></pre></td></tr></table></figure>
<p>&nbsp;   <!-- 空行 --></p>
<p><strong>PS：</strong> 这篇当时发在CSDN，后来有人给我评论感谢我，嘿嘿嘿<br><img src="/2021/03/09/008-%E7%BB%8F%E9%AA%8C%E6%95%99%E8%AE%AD/mmcv.png" class title="This is an test image"></p>
<h2 id="TypeError-’builtin-function-or-method’-object-has-no-attribute-‘getitem’"><a href="#TypeError-’builtin-function-or-method’-object-has-no-attribute-‘getitem’" class="headerlink" title="TypeError:’builtin_function_or method’ object has no attribute ‘getitem’"></a>TypeError:’builtin_function_or method’ object has no attribute ‘getitem’</h2><h3 id="原因分析："><a href="#原因分析：" class="headerlink" title="原因分析："></a>原因分析：</h3><font color="#999AAA">提示：内置函数或方法对象没有获取项目属性
可能是内置的函数中符号写错，比如range(1,5)写成range[1,5]</font>



<h2 id="except-more-than-1-value-per-channel-when-training，got-input-size……"><a href="#except-more-than-1-value-per-channel-when-training，got-input-size……" class="headerlink" title="except more than 1 value per channel when training，got input size……"></a>except more than 1 value per channel when training，got input size……</h2><p> &lt;/font&gt;</p>
<h3 id="原因分析：-1"><a href="#原因分析：-1" class="headerlink" title="原因分析："></a>原因分析：</h3><font color="#999AAA">当batch里只有一个样本时，调用batch_norm就会报错（bn层需要大于一个样本去计算其中的参数，需要多于一个数据计算平均值）
比如dataset的总样本数为17，而batch_size为8，就会报错</font>


<h3 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h3><ul>
<li><p>在训练前用num_of_samples%batch_size算一下会不会正好剩下一个样本</p>
</li>
<li><p>将dataloader的丢弃参数设置为true，drop_last=True</p>
</li>
</ul>
<h2 id="训练时出现nan"><a href="#训练时出现nan" class="headerlink" title="训练时出现nan"></a>训练时出现nan</h2><h3 id="原因分析：-2"><a href="#原因分析：-2" class="headerlink" title="原因分析："></a>原因分析：</h3><font color="#999AAA">梯度爆炸，调小学习率，加BN，梯度裁剪；损失函数或网络设计，出现除0</font>



<h2 id="Error-running’train’-Unknown-error-4-minutes-ago"><a href="#Error-running’train’-Unknown-error-4-minutes-ago" class="headerlink" title="Error running’train’:Unknown error(4 minutes ago)"></a>Error running’train’:Unknown error(4 minutes ago)</h2><p>waiting for process detach—————————————<br>运行界面无任何东西<br>且run界面经常几个程序的结果图片拼凑在一起</p>
<p>网上同样问题： <a href="https://stackoverflow.com/questions/67209905/why-keep-getting-the-same-error-in-pycharm-error-running-unknown-error">https://stackoverflow.com/questions/67209905/why-keep-getting-the-same-error-in-pycharm-error-running-unknown-error</a><br> &lt;/font&gt;</p>
<h3 id="解决方案：-1"><a href="#解决方案：-1" class="headerlink" title="解决方案："></a>解决方案：</h3><p>1、工具栏——Run——Edit Configurations</p>
<p>2、删除出问题的项目路径：</p>
<p>3、保存后，重新运行项目，就正常了。</p>
<h2 id="RuntimeError-cuDNN-error-CUDNN-STATUS-NOT-INITIALIZED"><a href="#RuntimeError-cuDNN-error-CUDNN-STATUS-NOT-INITIALIZED" class="headerlink" title="RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED"></a>RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED</h2><h3 id="原因分析：-3"><a href="#原因分析：-3" class="headerlink" title="原因分析："></a>原因分析：</h3><font color="#999AAA">1、第一个办法就是重启大法；
2、查看cuda版本与cudnn版本是否匹配；
3、查看pytorch版本是否与显卡匹配，不知道是否匹配，就试试，或许降低版本会有意料之外的效果，即解决了；
4、看驱动版本是否与cuda匹配；</font>

<h3 id="解决方案：-2"><a href="#解决方案：-2" class="headerlink" title="解决方案："></a>解决方案：</h3><p>1、删掉重装<br>pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f <a href="https://download.pytorch.org/whl/torch_stable.html">https://download.pytorch.org/whl/torch_stable.html</a><br>cpu/torch-0.3.0.post4-cp27-cp27m-linux_x86_64.whl cpu/torch-0.3.0.post4-cp27-cp27mu-linux_x86_64.whl</p>
<h2 id="Pycharm-Can’t-get-remote-credentials-for-deployment-server"><a href="#Pycharm-Can’t-get-remote-credentials-for-deployment-server" class="headerlink" title="Pycharm:Can’t get remote credentials for deployment server"></a>Pycharm:Can’t get remote credentials for deployment server</h2><p>在使用pycharm的过程中，在Path mappings栏，无法点击右边的按钮<br>但是在deployment中，是确定已经连接上服务器的，而且Run&gt;Edit configuration中，Python interpreter也是远程服务器上的解释器.<br> &lt;/font&gt;</p>
<h3 id="原因分析：-4"><a href="#原因分析：-4" class="headerlink" title="原因分析："></a>原因分析：</h3><h3 id="解决方案：-3"><a href="#解决方案：-3" class="headerlink" title="解决方案："></a>解决方案：</h3><p>把图片上显示的Python interpreter删除，然后重新设置，然后就可以正常使用了。</p>
<h2 id="RuntimeError-CUDA-error-out-of-memory"><a href="#RuntimeError-CUDA-error-out-of-memory" class="headerlink" title="RuntimeError:CUDA error:out of memory"></a>RuntimeError:CUDA error:out of memory</h2><h3 id="原因分析：-5"><a href="#原因分析：-5" class="headerlink" title="原因分析："></a>原因分析：</h3><p>可能是tensorflow和pytorch版本冲突导致的，需要让pytorch能看到服务器上所有gpu<br><img src="https://img-blog.csdnimg.cn/20210707215310620.png" alt="在这里插入图片描述"></p>
<p>正常的out of memory是<br>RuntimeError: CUDA out of memory. Tried to allocate 1.27 GiB (GPU 0; 11.75 GiB total capacity; 10.14 GiB already allocated; 543.75 MiB free; 18.10 MiB cached)</p>
<h3 id="解决方案：-4"><a href="#解决方案：-4" class="headerlink" title="解决方案："></a>解决方案：</h3><p>链接：  <a href="https://github.com/pytorch/pytorch/issues/20990">https://github.com/pytorch/pytorch/issues/20990</a></p>
<p>在代码前面加上：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.cuda.current_device()</span><br><span class="line">torch.cuda._initialized = <span class="literal">True</span></span><br></pre></td></tr></table></figure></p>
<h2 id="IndexError-tuple-index-out-of-range"><a href="#IndexError-tuple-index-out-of-range" class="headerlink" title="IndexError: tuple index out of range"></a>IndexError: tuple index out of range</h2><p>File “/storage/node3/hyy/anaconda3/lib/python3.6/site-packages/torchsummary/torchsummary.py”, line 19, in hook<br>    summary[m_key][“input_shape”] = list(input[0].size())</p>
<p>RuntimeError: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu:134</p>
<h3 id="原因分析：-6"><a href="#原因分析：-6" class="headerlink" title="原因分析："></a>原因分析：</h3><p>a = torch.randn(10)<br>a[11]<br>这里的11超过了a的范围</p>
<h2 id="RuntimeError-cuda-runtime-error-710-device-side-assert-triggered-at-pytorch-aten-src-THCUNN-generic-SpatialClassNLLCriterion-cu-134"><a href="#RuntimeError-cuda-runtime-error-710-device-side-assert-triggered-at-pytorch-aten-src-THCUNN-generic-SpatialClassNLLCriterion-cu-134" class="headerlink" title="RuntimeError: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu:134"></a>RuntimeError: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu:134</h2><h3 id="原因分析：-7"><a href="#原因分析：-7" class="headerlink" title="原因分析："></a>原因分析：</h3><ol>
<li>计算loss前没有sigmoid或者softmax</li>
<li>label标签为负值或者大于类别数，专业解释：训练过程中存在超出分类数目的标签，索引异常<br>因为标签要满足t &gt;= 0 &amp;&amp; t &lt; n_classes。而我的不是因为出现-1标签，是因为超出了类别数的范围</li>
<li>输出的通道数不对（PraNet）</li>
</ol>
<h3 id="解决方案：-5"><a href="#解决方案：-5" class="headerlink" title="解决方案："></a>解决方案：</h3><p>1.PraNet的深度监督每个输出前加个卷积使其变成三通道</p>
<h2 id="RuntimeError-DataLoader-worker-pid-644988-is-killed-by-signal-Segmentation-fault"><a href="#RuntimeError-DataLoader-worker-pid-644988-is-killed-by-signal-Segmentation-fault" class="headerlink" title="RuntimeError: DataLoader worker (pid 644988) is killed by signal: Segmentation fault."></a>RuntimeError: DataLoader worker (pid 644988) is killed by signal: Segmentation fault.</h2><h3 id="原因分析：-8"><a href="#原因分析：-8" class="headerlink" title="原因分析："></a>原因分析：</h3><p>数据加载过程中内存被耗尽</p>
<p>可通过减少dataloader的num_worker或增加虚拟内存解决。</p>
<h3 id="解决方案：-6"><a href="#解决方案：-6" class="headerlink" title="解决方案："></a>解决方案：</h3><h2 id="Pycharm报错：Process-finished-with-exit-code-134-interrupted-by-signal-6-SIGABRT"><a href="#Pycharm报错：Process-finished-with-exit-code-134-interrupted-by-signal-6-SIGABRT" class="headerlink" title="Pycharm报错：Process finished with exit code 134 (interrupted by signal 6: SIGABRT)"></a>Pycharm报错：Process finished with exit code 134 (interrupted by signal 6: SIGABRT)</h2><h3 id="原因分析：-9"><a href="#原因分析：-9" class="headerlink" title="原因分析："></a>原因分析：</h3><h3 id="解决方案：-7"><a href="#解决方案：-7" class="headerlink" title="解决方案："></a>解决方案：</h3><p><a href="https://blog.csdn.net/m0_43505377/article/details/103945714">https://blog.csdn.net/m0_43505377/article/details/103945714</a></p>
<p><a href="https://blog.csdn.net/jizhidexiaoming/article/details/80918868">https://blog.csdn.net/jizhidexiaoming/article/details/80918868</a></p>
<p>&lt;hr style=” border:solid; width:100px; height:1px;” color=#000000 size=1”&gt;</p>
<h2 id="RuntimeError-cuDNN-error-CUDNN-STATUS-NOT-INITIALIZED-1"><a href="#RuntimeError-cuDNN-error-CUDNN-STATUS-NOT-INITIALIZED-1" class="headerlink" title="RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED"></a>RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED</h2><h3 id="原因分析：-10"><a href="#原因分析：-10" class="headerlink" title="原因分析："></a>原因分析：</h3><p>GPU算力不足</p>
<h2 id="Host-key-verification-failed"><a href="#Host-key-verification-failed" class="headerlink" title="Host key verification failed."></a>Host key verification failed.</h2><h3 id="原因分析：-11"><a href="#原因分析：-11" class="headerlink" title="原因分析："></a>原因分析：</h3><p>这个问题，是你重置过你的服务器后。你再次想访问会出现这个问题。</p>
<h3 id="解决方案：-8"><a href="#解决方案：-8" class="headerlink" title="解决方案："></a>解决方案：</h3><p>ssh-keygen -R 你要访问的IP地址<br>比如：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ssh-keygen -R  <span class="number">172.21</span><span class="number">.144</span><span class="number">.46</span></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>经验记录</category>
      </categories>
      <tags>
        <tag>Bug</tag>
      </tags>
  </entry>
  <entry>
    <title>Python&amp;&amp;Pytorch笔记</title>
    <url>/2021/08/04/009-Python-Pytorch%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><strong> 前言 </strong></p>
<font color="#999AAA">
写代码看代码遇到的问题</font>

<h1 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h1><h2 id="map-and-lambda"><a href="#map-and-lambda" class="headerlink" title="map and lambda"></a>map and lambda</h2><p><strong>描述</strong><br>map() 会根据提供的函数对指定序列做映射。</p>
<p>第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。</p>
<p><strong>语法</strong><br>map() 函数语法：</p>
<p>map(function, iterable, …)<br><strong>参数</strong></p>
<ul>
<li>function — 函数</li>
<li>iterable — 一个或多个序列</li>
</ul>
<p><strong>返回值</strong><br>Python 2.x 返回列表。<br>Python 3.x 返回迭代器。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">def</span> <span class="title function_">square</span>(<span class="params">x</span>) :            <span class="comment"># 计算平方数</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> x ** <span class="number">2</span></span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">map</span>(square, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])   <span class="comment"># 计算列表各个元素的平方</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">map</span>(<span class="keyword">lambda</span> x: x ** <span class="number">2</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])  <span class="comment"># 使用 lambda 匿名函数</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>]</span><br><span class="line"><span class="comment"># 提供了两个列表，对相同位置的列表数据进行相加</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">map</span>(<span class="keyword">lambda</span> x, y: x + y, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>])</span><br><span class="line">[<span class="number">3</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">19</span>]</span><br></pre></td></tr></table></figure>
<p><strong>lambda表达式是一行函数。</strong><br>它们在其他语言中也被称为匿名函数。如果你不想在程序中对一个函数使用两次，你也许会想用lambda表达式，它们和普通的函数完全一样。</p>
<p><strong>原型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lambda</span> 参数:操作(参数)</span><br></pre></td></tr></table></figure>
<p><strong>例子</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add = <span class="keyword">lambda</span> x, y: x + y</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(add(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># Output: 8</span></span><br></pre></td></tr></table></figure></p>
<p>这还有一些lambda表达式的应用案例，可以在一些特殊情况下使用：</p>
<p><strong>列表排序</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">9</span>, <span class="number">10</span>), (<span class="number">13</span>, -<span class="number">3</span>)]</span><br><span class="line">a.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># Output: [(13, -3), (4, 1), (1, 2), (9, 10)]</span></span><br></pre></td></tr></table></figure>
<p><strong>列表并行排序</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = <span class="built_in">zip</span>(list1, list2)</span><br><span class="line">data = <span class="built_in">sorted</span>(data)</span><br><span class="line">list1, list2 = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: <span class="built_in">list</span>(t), <span class="built_in">zip</span>(*data))</span><br></pre></td></tr></table></figure>
<h2 id="nonzero"><a href="#nonzero" class="headerlink" title="nonzero()"></a>nonzero()</h2><p>用于得到numpy数组中非零元素的位置（数组索引）的函数</p>
<p>返回值是一个长度为a.ndim数组a的轴数的元组，元组的每个元素都是一个整数数组，其值为非零元素的下标在对应轴上的值</p>
<h2 id="setattr"><a href="#setattr" class="headerlink" title="setattr()"></a>setattr()</h2><p>setattr() 函数指定对象的指定属性的值。<br>改变 “person” 对象的 “age” 属性的值：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Person</span>:</span><br><span class="line">  name = <span class="string">&quot;John&quot;</span></span><br><span class="line">  age = <span class="number">36</span></span><br><span class="line">  country = <span class="string">&quot;Norway&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">setattr</span>(Person, <span class="string">&#x27;age&#x27;</span>, <span class="number">40</span>)</span><br></pre></td></tr></table></figure><br>getattr() 函数获取某个类实例对象中指定属性的值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">getattr</span>(clangs,<span class="string">&quot;name&quot;</span>))</span><br></pre></td></tr></table></figure>
<h1 id="import-torch"><a href="#import-torch" class="headerlink" title="import torch"></a>import torch</h1><h2 id="torch-roll"><a href="#torch-roll" class="headerlink" title="torch.roll"></a>torch.roll</h2><p><font color="#999AAA"> torch.roll(input, shifts, dims=None) → Tensor</font></p>
<p><font color="#999AAA"><em>Roll the tensor along the given dimension(s). Elements that are shifted beyond the last position are re-introduced at the first position. If a dimension is not specified, the tensor will be flattened before rolling and then restored to the original shape.</em></font></p>
<p>Parameters</p>
<ul>
<li><strong>input</strong> (<a href="https://pytorch.org/docs/master/tensors.html#torch.Tensor"><em>Tensor</em></a>) – the input tensor.</li>
<li><strong>shifts</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <em>tuple of python:ints</em>) – The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value</li>
<li><strong>dims</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <em>tuple of python:ints</em>) – Axis along which to roll</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]).view(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.roll(x, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">tensor([[<span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.roll(x, -<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">tensor([[<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.roll(x, shifts=(<span class="number">2</span>, <span class="number">1</span>), dims=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">tensor([[<span class="number">6</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="torch-nn-Parameter"><a href="#torch-nn-Parameter" class="headerlink" title="torch.nn.Parameter()"></a>torch.nn.Parameter()</h2><p>self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))</p>
<h1 id="将一个不可训练的类型Tensor转换成可以训练的类型parameter，并将parameter绑定到module里（可以优化）"><a href="#将一个不可训练的类型Tensor转换成可以训练的类型parameter，并将parameter绑定到module里（可以优化）" class="headerlink" title="将一个不可训练的类型Tensor转换成可以训练的类型parameter，并将parameter绑定到module里（可以优化）"></a>将一个不可训练的类型Tensor转换成可以训练的类型parameter，并将parameter绑定到module里（可以优化）</h1><p>与torch.tensor([1,2,3],requires_grad=True)的区别，这个只是将参数变成可训练的，并没有绑定在module的parameter列表中。</p>
<h2 id="torch-contiguous"><a href="#torch-contiguous" class="headerlink" title="torch.contiguous()"></a>torch.contiguous()</h2><p>torch.contiguous()方法语义上是“连续的”，经常与torch.permute()、torch.transpose()、torch.view()方法一起使用</p>
<h2 id="touch-view-方法"><a href="#touch-view-方法" class="headerlink" title="touch.view()方法"></a>touch.view()方法</h2><p>对张量改变“形状”其实并没有改变张量在内存中真正的形状，可以理解为：</p>
<ol>
<li>view方法没有拷贝新的张量，没有开辟新内存，与原张量共享内存；</li>
<li>view方法只是重新定义了访问张量的规则，使得取出的张量按照我们希望的形状展现。</li>
</ol>
<p>按照行优先原则，数字在语义和在内存中都是连续的，当我们使用torch.transpose()方法或者torch.permute()方法对张量翻转后，改变了张量的形状</p>
<p>此时如果对t2使用view方法，会报错：</p>
<p>原因是：改变了形状的t2语义上是3行2列的，在内存中还是跟t一样，没有改变，导致如果按照语义的形状进行view拉伸，数字不连续，此时torch.contiguous()方法就派上用场了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>x.contiguous().view<br>把原先tensor中的数据按照行优先顺序排成一个一维的数据，然后按照参数组合成其他维度的tensor，参数不可为空，-1代表这个位置可以由其他位置的数字来推断</p>
</blockquote>
<h2 id="torch-chunk"><a href="#torch-chunk" class="headerlink" title="torch.chunk()"></a>torch.chunk()</h2><p>chunk方法可以对张量分块，返回一个张量列表：</p>
<p>torch.chunk(tensor, chunks, dim=0) → List of Tensors</p>
<blockquote>
<p>Splits a tensor into a specific number of chunks.</p>
<p>Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks.<br>（如果指定轴的元素个数被chunks除不尽，那么最后一块的元素个数变少）</p>
</blockquote>
<p>Parameters:    </p>
<ul>
<li>tensor (Tensor) – the tensor to split,待分割张量</li>
<li>chunks (int) – number of chunks to return（分割的块数）</li>
<li>dim (int) – dimension along which to split the tensor（沿着哪个轴分块）</li>
</ul>
<h2 id="torch-einsum-爱因斯坦简记法"><a href="#torch-einsum-爱因斯坦简记法" class="headerlink" title="torch.einsum 爱因斯坦简记法"></a>torch.einsum 爱因斯坦简记法</h2><p>对向量、矩阵、张量的求和运算<br>torch.einsum(‘bhid,bhjd-&gt;bhij’,q,k) * self.scale<br>输入q，k，沿下标id</p>
<h2 id="torch-split-将tensor分成块结构"><a href="#torch-split-将tensor分成块结构" class="headerlink" title="torch.split 将tensor分成块结构"></a>torch.split 将tensor分成块结构</h2><p>torch.split(tensor, split_size_or_sections, dim=0)<br>参数：</p>
<ul>
<li>tesnor：input，待分输入</li>
<li>split_size_or_sections：需要切分的大小(int or list )</li>
<li>dim：切分维度</li>
<li>output：切分后块结构 <class 'tuple'></class></li>
</ul>
<blockquote>
<p>当split_size_or_sections为int时，tenor结构和split_size_or_sections，正好匹配，那么ouput就是大小相同的块结构。如果按照split_size_or_sections结构，tensor不够了，那么就把剩下的那部分做一个块处理。<br>当split_size_or_sections为list时，那么tensor结构会一共切分成len(list)这么多的小块，每个小块中的大小按照list中的大小决定，其中list中的数字总和应等于该维度的大小，否则会报错（注意这里与split_size_or_sections为int时的情况不同）。</p>
</blockquote>
<p>split_size_or_sections为int型时</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line">x = torch.rand(<span class="number">4</span>,<span class="number">8</span>,<span class="number">6</span>)</span><br><span class="line">y = torch.split(x,<span class="number">2</span>,dim=<span class="number">0</span>) <span class="comment">#按照4这个维度去分，每大块包含2个小块</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y :</span><br><span class="line">    <span class="built_in">print</span>(i.size())</span><br><span class="line"> </span><br><span class="line">output:</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">8</span>, <span class="number">6</span>])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">8</span>, <span class="number">6</span>])</span><br><span class="line"> </span><br><span class="line">y = torch.split(x,<span class="number">3</span>,dim=<span class="number">0</span>)<span class="comment">#按照4这个维度去分，每大块包含3个小块</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y:</span><br><span class="line">    <span class="built_in">print</span>(i.size())</span><br><span class="line"> </span><br><span class="line">output:</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">8</span>, <span class="number">6</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">8</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>
<p>  split_size_or_sections为list型时</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line">x = torch.rand(<span class="number">4</span>,<span class="number">8</span>,<span class="number">6</span>)</span><br><span class="line">y = torch.split(x,[<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>],dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y:</span><br><span class="line">    <span class="built_in">print</span>(i.size())</span><br><span class="line"> </span><br><span class="line">output:</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">y = torch.split(x,[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>],dim=<span class="number">1</span>) <span class="comment">#2+1+3 等于7,报错</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y:</span><br><span class="line">    <span class="built_in">print</span>(i.size())</span><br><span class="line"> </span><br><span class="line">output:</span><br><span class="line">split_with_sizes expects split_sizes to <span class="built_in">sum</span> exactly to <span class="number">8</span> (<span class="built_in">input</span> tenso<span class="string">r&#x27;s size at dimension 1), but got split_sizes=[2, 1, 3]</span></span><br></pre></td></tr></table></figure>
<h1 id="import-torch-nn"><a href="#import-torch-nn" class="headerlink" title="import torch.nn"></a>import torch.nn</h1><h2 id="nn-Conv2d-groups-in-channels"><a href="#nn-Conv2d-groups-in-channels" class="headerlink" title="nn.Conv2d(groups=in_channels)"></a>nn.Conv2d(groups=in_channels)</h2><p>Depthwise COnv 深度卷积，意思是将输入的每一个通道作为一组，然后分别对其卷积</p>
<h2 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a>nn.ModuleList</h2><p>它是一个储存不同nn.module，并自动将每个 module 的 parameters 添加到网络之中的容器(extend 和 append方法)。但是，nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer_branch</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer_branch, self).__init__()</span><br><span class="line">        self.encoder_norm = LayerNorm(config.dim, eps=<span class="number">1e-6</span>) <span class="comment">#config.dim=256</span></span><br><span class="line">        self.embedding = Embeddings(config)</span><br><span class="line">        self.layer = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(config.transformer[<span class="string">&quot;num_layers&quot;</span>]):</span><br><span class="line">            layer = Att_block(config)</span><br><span class="line">            self.layer.append(copy.deepcopy(layer))</span><br><span class="line">        self.decoder = Decoder_vit(config)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        <span class="keyword">for</span> layer_block <span class="keyword">in</span> self.layer:</span><br><span class="line">            x = layer_block(x)</span><br><span class="line">        encoded = self.encoder_norm(x)</span><br><span class="line">        x_16, x_8, x_4  = self.decoder(encoded)</span><br><span class="line">        <span class="keyword">return</span> x_16, x_8, x_4</span><br></pre></td></tr></table></figure>
<p>注意：nn.Sequential内部实现了forward函数，而nn.ModuleList需要写forward函数</p>
<h2 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h2><p>不同于 nn.ModuleList，它已经实现的 forward 函数，而且里面的模块是按照顺序进行排列的，所以我们必须确保前一个模块的输出大小和下一个模块的输入大小是一致的</p>
<h2 id="make-layer"><a href="#make-layer" class="headerlink" title="_make_layer"></a>_make_layer</h2><p>对于一些重复的layer可以采用nn.Sequential先进行封装layer，然后调用<em>make_layer</em>进行实现<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,block,layers,num_classes=<span class="number">1000</span></span>):</span><br><span class="line"> </span><br><span class="line">    self.in_channels = <span class="number">64</span></span><br><span class="line">    <span class="built_in">super</span>(ResNet,self).__init__()</span><br><span class="line">    self.conv1 = nn.Conv2d(</span><br><span class="line">        <span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>,bias=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line">    self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">    self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>)</span><br><span class="line">    self.layer1 = self._make_layer(block,<span class="number">64</span>,layers[<span class="number">0</span>])</span><br><span class="line">    self.layer2 = self._make_layer(block,<span class="number">128</span>,layers[<span class="number">1</span>],stride=<span class="number">2</span>)</span><br><span class="line">    self.layer3 = self._make_layer(block,<span class="number">256</span>,layers[<span class="number">2</span>],stride=<span class="number">2</span>)</span><br><span class="line">    self.layer4 = self._make_layer(block,<span class="number">512</span>,layers[<span class="number">3</span>],stride=<span class="number">2</span>)</span><br><span class="line">    self.avgpool = nn.AvgPool2d(<span class="number">7</span>,stride=<span class="number">1</span>)</span><br><span class="line">    self.fc = nn.Linear(<span class="number">512</span>*block.expansion,num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self,block,out_channels,blocks,stride=<span class="number">1</span></span>):</span><br><span class="line">    downsample = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> stride!=<span class="number">1</span> <span class="keyword">or</span> self.in_channels != out_channels*block.expansion:</span><br><span class="line"> </span><br><span class="line">        downsample = nn.Sequential(</span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                self.in_channels,out_channels*block.expansion,kernel_size=<span class="number">1</span>,</span><br><span class="line">                stride=stride,bias=<span class="literal">False</span> </span><br><span class="line">            ),</span><br><span class="line">            nn.BatchNorm2d(out_channels*block.expansion)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    layers = []</span><br><span class="line">    <span class="comment">## 虚线部分</span></span><br><span class="line">    layers.append(block(self.in_channels,out_channels,stride,downsample))</span><br><span class="line">    <span class="comment">## 更新 in_channels</span></span><br><span class="line">    self.in_channels = out_channels*block.expansion </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,blocks):</span><br><span class="line">        layers.append(block(self.in_channels,out_channels))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line"> </span><br><span class="line">    x = self.conv1(x)</span><br><span class="line">    x = self.bn1(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.maxpool(x)</span><br><span class="line"> </span><br><span class="line">    x2 = self.layer1(x)</span><br><span class="line">    x3 = self.layer2(x2)</span><br><span class="line">    x4 = self.layer3(x3)</span><br><span class="line"> </span><br><span class="line">    x5 = self.layer4(x4)</span><br><span class="line">    x6 = self.avgpool(x5)</span><br><span class="line">    x7 = x6.view(x6.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> self.fc(x7)</span><br></pre></td></tr></table></figure></p>
<h2 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>nn1 = torch.nn.Linear(<span class="number">100</span>, <span class="number">50</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input1 = torch.randn(<span class="number">140</span>, <span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output1 = nn1(input1)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output1.size()</span><br><span class="line">torch.Size([<span class="number">140</span>, <span class="number">50</span>])</span><br></pre></td></tr></table></figure>
<p>张量的大小由 140 x 100 变成了 140 x 50</p>
<p>执行的操作是：</p>
<p><font color="#999AAA"> <em>[140,100]×[100,50]=[140,50]</em></font></p>
<h2 id="nn-Unfold"><a href="#nn-Unfold" class="headerlink" title="nn.Unfold"></a>nn.Unfold</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.Unfold(kernel_size,dilation=<span class="number">1</span>,padding=<span class="number">0</span>,stride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>从一个batch的样本中，提取出滑动的局部区域块（类似卷积操作提取kernel filter对应的滑动窗口）<br>(N,C,H,W)  N=batch_size,C=channel个数<br>输出 （N,C*kernel_size长和宽乘积,L）  L是滑动剪裁后得到的区块的数量</p>
<p><strong>torch.nn.Fold</strong>   将提取出的滑动局部区域还原成batch张量</p>
<h2 id="共享权重shared-weight"><a href="#共享权重shared-weight" class="headerlink" title="共享权重shared weight"></a>共享权重shared weight</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">branches_1 =  self.conv3*<span class="number">3</span>(x)</span><br><span class="line"></span><br><span class="line">branches_2 = F.conv2d(x,self.conv3*<span class="number">3.</span>weight,padding=<span class="number">2</span>,dilation=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h2><p>BN，LN，IN，GN从学术化上解释差异：</p>
<ul>
<li>BatchNorm：batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布</li>
</ul>
<ul>
<li>LayerNorm：channel方向做归一化，算CHW的均值，主要对RNN作用明显；<br>InstanceNorm：一个channel内做归一化，算H*W的均值，用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。</li>
<li>GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束。<br>SwitchableNorm是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。</li>
</ul>
<h2 id="permute-维度换位"><a href="#permute-维度换位" class="headerlink" title="permute 维度换位"></a>permute 维度换位</h2><p>(28,28,3) img.permute(2,0,1)   (3,28,28) </p>
<h2 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t(<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">&gt; t.flatten(start_dim=<span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">16</span>])</span><br></pre></td></tr></table></figure>
<h2 id="Drop-path"><a href="#Drop-path" class="headerlink" title="Drop_path"></a>Drop_path</h2><p>将深度学习模型中的多分支结构随机删除</p>
<h1 id="import-其他包"><a href="#import-其他包" class="headerlink" title="import 其他包"></a>import 其他包</h1><h2 id="copy"><a href="#copy" class="headerlink" title="copy"></a>copy</h2><p>我们寻常意义的复制就是深复制，即将被复制对象完全再复制一遍作为独立的新个体单独存在。所以改变原有被复制对象不会对已经复制出来的新对象产生影响。<br>—–而浅复制并不会产生一个独立的对象单独存在，他只是将原有的数据块打上一个新标签，所以当其中一个标签被改变的时候，数据块就会发生变化，另一个标签也会随之改变。这就和我们寻常意义上的复制有所不同了。</p>
<p>对于简单的 object，用 shallow copy 和 deep copy 没区别</p>
<p>复杂的 object， 如 list 中套着 list 的情况，shallow copy 中的 子list，并未从原 object 真的「独立」出来。也就是说，如果你改变原 object 的子 list 中的一个元素，你的 copy 就会跟着一起变。这跟我们直觉上对「复制」的理解不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> copy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>origin = [<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line"><span class="comment">#origin 里边有三个元素：1， 2，[3, 4]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cop1 = copy.copy(origin)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cop2 = copy.deepcopy(origin)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cop1 == cop2</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cop1 <span class="keyword">is</span> cop2</span><br><span class="line"><span class="literal">False</span> </span><br><span class="line"><span class="comment">#cop1 和 cop2 看上去相同，但已不再是同一个object</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>origin[<span class="number">2</span>][<span class="number">0</span>] = <span class="string">&quot;hey!&quot;</span> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>origin</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, [<span class="string">&#x27;hey!&#x27;</span>, <span class="number">4</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cop1</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, [<span class="string">&#x27;hey!&#x27;</span>, <span class="number">4</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cop2</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3</span>, <span class="number">4</span>]]</span><br></pre></td></tr></table></figure>
<p>把origin内的子list [3, 4] 改掉了一个元素，观察 cop1 和 cop2<br>更详细地请参考 <a href="https://blog.csdn.net/u010712012/article/details/79754132">https://blog.csdn.net/u010712012/article/details/79754132</a></p>
<h2 id="ml-connections"><a href="#ml-connections" class="headerlink" title="ml_connections"></a>ml_connections</h2><p>ML Collections是为ML use cases而设计的一个Python Collections的一个库。它的两个类是ConfigDict和FrozenConfigDict，是”dict-like” 的数据结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> ml_collections</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_config</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns the ViT configuration.&quot;&quot;&quot;</span></span><br><span class="line">    config = ml_collections.ConfigDict()</span><br><span class="line">    config.transformer = ml_collections.ConfigDict()</span><br><span class="line">    config.transformer.mlp_dim = <span class="number">786</span></span><br><span class="line"><span class="built_in">print</span>(config.integer_field) <span class="comment"># 输出结果 123.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(config[<span class="string">&#x27;integer_field&#x27;</span>]) <span class="comment"># 也输出123.</span></span><br></pre></td></tr></table></figure>
<h2 id="einops-处理张量维度"><a href="#einops-处理张量维度" class="headerlink" title="einops 处理张量维度"></a>einops 处理张量维度</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, reduce, repeat</span><br><span class="line"></span><br><span class="line"><span class="comment"># rearrange elements according to the pattern</span></span><br><span class="line">output_tensor = rearrange(input_tensor, <span class="string">&#x27;t b c -&gt; b c t&#x27;</span>)</span><br><span class="line"><span class="comment"># combine rearrangement and reduction</span></span><br><span class="line">output_tensor = reduce(input_tensor, <span class="string">&#x27;b c (h h2) (w w2) -&gt; b h w c&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>, h2=<span class="number">2</span>, w2=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># copy along a new axis </span></span><br><span class="line">output_tensor = repeat(input_tensor, <span class="string">&#x27;h w -&gt; h w c&#x27;</span>, c=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原文链接：https://blog.csdn.net/ViatorSun/article/details/116010049</span></span><br></pre></td></tr></table></figure>
<h3 id="rearrange-维度调整"><a href="#rearrange-维度调整" class="headerlink" title="rearrange 维度调整"></a>rearrange 维度调整</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rearrange(ims[<span class="number">0</span>], <span class="string">&#x27;h w c -&gt; w h c&#x27;</span>)		<span class="comment"># 调换维度</span></span><br><span class="line"></span><br><span class="line">rearrange(ims, <span class="string">&#x27;b h w c -&gt; (b h) w c&#x27;</span>)  <span class="comment"># 合并维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or compose a new dimension of batch and width</span></span><br><span class="line">rearrange(ims, <span class="string">&#x27;b h w c -&gt; h (b w) c&#x27;</span>)</span><br><span class="line">rearrange(ims, <span class="string">&#x27;b h w c -&gt; h (b w) c&#x27;</span>).shape</span><br><span class="line"></span><br><span class="line">[<span class="number">6</span>, <span class="number">96</span>, <span class="number">96</span>, <span class="number">3</span>] -&gt; [<span class="number">96</span>, (<span class="number">6</span> * <span class="number">96</span>), <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.mean(-<span class="number">1</span>)</span><br><span class="line">reduce(x, <span class="string">&#x27;b h w c -&gt; b h w&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"><span class="comment"># average over batch</span></span><br><span class="line">reduce(ims, <span class="string">&#x27;b h w c -&gt; h w c&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"><span class="comment"># this is mean-pooling with 2x2 kernel</span></span><br><span class="line"><span class="comment"># image is split into 2x2 patches, each patch is averaged</span></span><br><span class="line"><span class="comment"># 变小了</span></span><br><span class="line">reduce(ims, <span class="string">&#x27;b (h h2) (w w2) c -&gt; h (b w) c&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>, h2=<span class="number">2</span>, w2=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># yet another example. Can you compute result shape?</span></span><br><span class="line">reduce(ims, <span class="string">&#x27;(b1 b2) h w c -&gt; (b2 h) (b1 w)&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>, b1=<span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="torch-Tensor-4种常见乘法"><a href="#torch-Tensor-4种常见乘法" class="headerlink" title="torch.Tensor 4种常见乘法"></a>torch.Tensor 4种常见乘法</h2><h3 id="点乘：即-，element-wise乘法"><a href="#点乘：即-，element-wise乘法" class="headerlink" title="点乘：即*，element-wise乘法"></a>点乘：即*，element-wise乘法</h3><ul>
<li>原则是如果a与b的size不同，则以某种方式将a或b进行复制，使得复制后的a和b的size相同，然后再将a和b做element-wise的乘法      标量</li>
<li><p>Tensor与标量k做*乘法的结果是Tensor的每个元素乘以k（相当于把k复制成与lhs大小相同，元素全为k的Tensor）.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a * <span class="number">2</span></span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">    [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">    [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>一维向量<br>Tensor与行向量做*乘法的结果是每列乘以行向量对应列的值（相当于把行向量的行复制，成为与lhs维度相同的Tensor）. 注意此时要求Tensor的列数与行向量的列数相等。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a * b</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a * b</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>]])</span><br></pre></td></tr></table></figure>
<ul>
<li>矩阵<br>经Arsmart在评论区提醒，增补一个矩阵 <em> 矩阵的例子，感谢Arsmart的热心评论！<br>如果两个二维矩阵A与B做点积A </em> B，则要求A与B的维度完全相同，即A的行数=B的行数，A的列数=B的列数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a * a</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">4</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure>
<h5 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h5><p>点积是broadcast的。broadcast是torch的一个概念，简单理解就是在一定的规则下允许高维Tensor和低维Tensor之间的运算。broadcast的概念稍显复杂，在此不做展开，可以参考官方文档关于broadcast的介绍. 在torch.matmul里会有关于broadcast的应用的一个简单的例子。<br>这里举一个点积broadcast的例子。在例子中，a是二维Tensor，b是三维Tensor，但是a的维度与b的后两位相同，那么a和b仍然可以做点积，点积结果是一个和b维度一样的三维Tensor，运算规则是：若c = a <em> b, 则c[i,</em>,<em>] = a </em> b[i, <em>, </em>]，即沿着b的第0维做二维Tensor点积，或者可以理解为运算前将a沿着b的第0维也进行了expand操作，即a = a.expand(b.size()); a * b。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>]],[[-<span class="number">1</span>,-<span class="number">2</span>],[-<span class="number">2</span>,-<span class="number">3</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a * b</span><br><span class="line">tensor([[[ <span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">     [ <span class="number">4</span>, <span class="number">9</span>]],</span><br><span class="line"></span><br><span class="line">    [[-<span class="number">1</span>, -<span class="number">4</span>],</span><br><span class="line">     [-<span class="number">4</span>, -<span class="number">9</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b * a</span><br><span class="line">tensor([[[ <span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">     [ <span class="number">4</span>, <span class="number">9</span>]],</span><br><span class="line"></span><br><span class="line">    [[-<span class="number">1</span>, -<span class="number">4</span>],</span><br><span class="line">     [-<span class="number">4</span>, -<span class="number">9</span>]]])</span><br></pre></td></tr></table></figure>
<p>其实，上面提到的二维Tensor点积标量、二维Tensor点积行向量，都是发生在高维向量和低维向量之间的，也可以看作是broadcast.</p>
<h3 id="torch-mul"><a href="#torch-mul" class="headerlink" title="torch.mul"></a>torch.mul</h3><p>官方文档关于torch.mul的介绍. 用法与*乘法相同，也是element-wise的乘法，也是支持broadcast的。</p>
<h3 id="torch-mm"><a href="#torch-mm" class="headerlink" title="torch.mm"></a>torch.mm</h3><p>官方文档关于torch.mm的介绍. 数学里的矩阵乘法，要求两个Tensor的维度满足矩阵乘法的要求.</p>
<h3 id="torch-matmul"><a href="#torch-matmul" class="headerlink" title="torch.matmul"></a>torch.matmul</h3><p>官方文档关于torch.matmul的介绍. torch.mm的broadcast版本.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.ones(<span class="number">5</span>,<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.matmul(a, b)</span><br><span class="line">tensor([[[<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>]],</span><br><span class="line"></span><br><span class="line">    [[<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>]],</span><br><span class="line">     </span><br><span class="line">    [[<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>]],</span><br><span class="line">     </span><br><span class="line">    [[<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>]],</span><br><span class="line">     </span><br><span class="line">    [[<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">     [<span class="number">4.</span>, <span class="number">4.</span>]]])</span><br></pre></td></tr></table></figure>
<h3 id="torch-broadcast-tensors"><a href="#torch-broadcast-tensors" class="headerlink" title="torch.broadcast_tensors()"></a>torch.broadcast_tensors()</h3><p>Broadcasting 是指，在运算中，不同大小的两个 array 应该怎样处理的操作。通常情况下，小一点的数组会被 broadcast 到大一点的，这样才能保持大小一致。Broadcasting 过程中的循环操作都在 C 底层进行，所以速度比较快。但也有一些情况下 Broadcasting 会带来性能上的下降。</p>
<p>两个 Tensors 只有在下列情况下才能进行 broadcasting 操作：</p>
<ul>
<li>每个 tensor 至少有一维</li>
<li>遍历所有的维度，从尾部维度开始，每个对应的维度大小<strong>要么相同，要么其中一个是 1，要么其中一个不存在</strong>。</li>
</ul>
<p>一个将tensor扩充的函数，可以传两个参数，并且最后得到的结果会变成第二个参数的形状，值是填充的第一个参数的值。</p>
<p>函数一共有两个返回结果，第二个结果的形状和第一个结果相同，但是填充的值都是0</p>
]]></content>
      <categories>
        <category>研究生期间</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态简单调研</title>
    <url>/2021/08/17/010-%E5%A4%9A%E6%A8%A1%E6%80%81%E7%AE%80%E5%8D%95%E8%B0%83%E7%A0%94/</url>
    <content><![CDATA[<h2 id="多模态融合"><a href="#多模态融合" class="headerlink" title="多模态融合"></a>多模态融合</h2><p>四个方面的多模态数据融合算法研究：<br>不完整模态分析融合<br>增量模态聚类融合<br>异构模态迁移融合<br>低维模态共享融合</p>
<p><img src="https://img-blog.csdnimg.cn/4afb8f5ddc804bd7bf60ab614d2a3d12.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>1.<br>早期融合，又称为特征融合，是指对模态进行特征提取之后立刻进行的一种融合方式。特征融合的优势在于可以在早期利用来自不同模态的多个特征之间的相关性，适用于模态之间高度相关的情况。例如，在结合语音识别的音频和视频特征时采用早期融合[6]。但对于特征的提取难度较大[7]，并不是最理想的融合方法。<br>2.<br>后期融合，也称为决策层融合，指的是在每种模态都做出决策（分类或回归）之后才进行的融合。进行后期融合，需要使用相应的模型对不相同的模态进行训练，再对这些模型输出的结果进行融合。与之前的早期融合作比较，该融合方式可以处理简单的数据异步性。另一个优势是允许使用最适合分析每种单一模态的方法，如音频使用<strong>隐马尔可夫模型</strong>(Hidden Markov Model，HMM)、图像使用可支持向量机(Support Vector Machines，SVM)。</p>
<p>根据具体融合操作不同，可以大致的划分为三种主要的方法：基于拼接和线性组合等简单融合操作的方法、基于注意力机制的融合方法和基于双线性池化的融合方法。这三种方法均是通过对特征向量进行相关操作达到多模态信息的融合及表达。</p>
<p>多模态网络架构主要分为三种，即协同架构、联合架构和编解码器架构。<br>1.协同架构<br><img src="https://img-blog.csdnimg.cn/3ef8a2ef7a684154a27d507a44aeb4e6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>2.联合架构<br><img src="https://img-blog.csdnimg.cn/318e0da4d8b04ffaacf329c5bc2d9b27.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>3.编解码器架构<br><img src="https://img-blog.csdnimg.cn/b941b0f85a76463f90bb43d8a0603689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>多模态融合方法分为模型无关的融合方法和模型相关方法两种。模型无关方法有早期、晚期、混合融合三种；模型相关方法包括多核学习方法、图像模型方法和神经网络方法三种。单一模态的表示、融合完成后信息的表示是融合过程的基础，确保特征提取及融合过程中信息的完整性是融合成功的关键。</p>
<p><img src="https://img-blog.csdnimg.cn/c5897396c790488c87b1da4f63513050.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/6cfe46475a654ecba08b2595b3e9ca71.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/fbc00ef07f2e401d9da528eea7fbd5f3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>现有的多模态数据融合分析方法：<br><strong>1、基于阶段的融合算法</strong>：<br>　　在数据挖掘任务的不同阶段使用不同的模态数据完成相应的融合分析。（可做先验算法）</p>
<p>　　特点：不需要模态数据间的一致性（即不同模态数据间处于松耦合）</p>
<p>　　eg：1、区域图划分与区域图构建（模态：路网和出租车轨迹）。2、好友推荐系统（模态：空间轨迹数据和空间点静态分类数据）<br><strong>2、基于特征的融合方法</strong><br>早的基于特征的融合方法，直接将所有模态特征进行串联融合，然后基于串联后<br>的特征完成聚类、分类等数据挖掘任务。由于不同模态的表示、分布和密度均可能不同，<br>简单的属性连接会忽略模态特有的统计属性和模态间的相关关系，并产生数据的冗余与<br>依赖［１９］。一些改进算法在目标函数中添加稀疏规则化因子控制融合特征的冗余，在模型<br>的训练中将冗余特征的权重近似分配为零［２（）］。<br>最近的研究中，更多方法利用深度神经网络学习不同模态数据的统一特征表示。深<br>度神经网络能够学习得到数据的多层表示与抽象，进而将数据转换成深度网络的高层抽<br>象特征，这种新的特征比传统手工特征更具表达能力</p>
<p><strong>3、基于语义的融合算法：</strong><br>        基于特征的融合方法只是将每个特征当做实数或分类值，而不关注每个特征的具体意义。与基于特征的融合方法不同，基于语义的方法理解每个模态的数据含义及不同模<br>态特征之间的关系，在数据融合过程中利用人类思考问题的方式抽象不同模态的语义含<br>义完成跨模态数据融合。现有的基于语义的融合算法大致分为<strong>共训练、多核学习、子空间学习、概率依赖和迁移学习五类</strong>。<br>        理解每个模态的数据含义及不同模态特征之间的关系，在数据融合过程中利用人类思考问题的方式抽象不同模态的语义含义完成跨模态数据融合。<br>　　现有的基于语义的融合算法大致分为：<br><strong>3.1、共训练方法：</strong><br>　　通过轮流训练使得两个模态数据的协同度最大。<br>　　　　三个需求（假设）：1、每个模态有充分的数据；2、基于共生特征两个模态的目标函数都能以较高概率预测到相同的数据类标签；3、给定类标签，模态间条件独立。</p>
<p><strong>3.2、多核学习方法：</strong><br>　　　　利用预定义的一组核函数学习一个基于核函数的优化的线性或非线性组合。<br>　　　　<img src="https://img-blog.csdnimg.cn/d454a01ea0fe4646b53e5e763b00650e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>多核学习算法是机器学习的一类算法，他们利用预定义的一组核函数学习一个基于核函数的优化的线性或非线性组合，完成具体数据任务分析。一个核是一个基于数据的假设，可能是一个相似性概念、一个分类器或者一个回归量。根据文献，多核学习有两种方式<br>１）不同核对应不同的相似性概念，学习函数挑选最好的核计算结果或整合所有核的计算结果，这种多核学习方法利用所有模态数据完成每个核的训练，不适用于多模态融合学习；<br>２）利用不同模态数据训练不同的核，因此整合所有核学习结果相当于对所有模态信息进行融合，现有的多核结果整合算法可分为三类：线性、非线性和数据依赖整合，文献［３］已经对现有的多核学习方法进行详细的分析总结。<br>多核学习用于多模态分析主要是因为多核学习中不同核自然的对应于不同的模态，对各核进行适当的整合能够提升学习结果的性能。例如，等人将多核学习应用到多模态情感识别和语义分析中［３６］，通过对语义、视频和文本三个模态特征使用不同的核得到比单核模态融合更好的分析结果。文献［３７］将多核学习应用到人脸识别，提出基于多核稀疏表示的分类学习算法，算法在多核空间内同时执行稀疏编码和字典学习，通过可能核合并和稀疏系数计算得到核的优化权重。</p>
<p><strong>3.3、子空间学习方法：</strong><br>　　　　假设所以模态均可以投影到同一语义共享子空间，在子空间内可以完成聚类、分类等数据挖掘任务。（共享子空间的特征维度小于任何一个模态数据维度（维度灾难））<br>　　　　<img src="https://img-blog.csdnimg.cn/56b93f95d230472ab24c24ecd023344a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>[1]张磊,赵耀,朱振峰.跨媒体语义共享子空间学习研究进展[J].计算机学报,2017,40(06):1394-1421.<br><img src="https://img-blog.csdnimg.cn/ac7b0ef8044d4ef6a972aeecb3ff1f21.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/f5ec4394bca84c72a6fba20a9ef7aea9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/1c01f90c360f4bf8baf4c466a508509c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/51cfc4eee10544029df2f7e21bebe3c2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/bbd53d0421484c7488ea8eaf3a429d19.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/c5a386d68dd7405dab1985de802aec49.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/567969a1e97d48a8b76db3f3a06aaf15.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/f949c43201d845e99d6cc4aa49ce3520.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/7a5497b0de4144949601d476357566f8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/f249f73f97834b0d848a1c3ce97ce71c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2939787aab794675b2dc07442e493712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>研究生期间</category>
      </categories>
      <tags>
        <tag>调研</tag>
        <tag>多模态</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ACM模板大全</title>
    <url>/2019/04/09/012-ACM%E6%A8%A1%E6%9D%BF%E5%A4%A7%E5%85%A8/</url>
    <content><![CDATA[<h3 id="线段树单点修改区间求和模板"><a href="#线段树单点修改区间求和模板" class="headerlink" title="线段树单点修改区间求和模板"></a>线段树单点修改区间求和模板</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> maxn=<span class="number">1005</span>;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> lson l,mid,rt&lt;&lt;1</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> rson mid+1,r,rt&lt;&lt;1|1</span></span><br><span class="line"><span class="type">int</span> sum[maxn&lt;&lt;<span class="number">2</span>],n;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">push_up</span><span class="params">(<span class="type">int</span> rt)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    sum[rt] = sum[rt&lt;&lt;<span class="number">1</span>]+sum[rt&lt;&lt;<span class="number">1</span>|<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">build</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r, <span class="type">int</span> rt)</span><span class="comment">//建树</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l==r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>,&amp;sum[rt]);</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> mid=l+r&gt;&gt;<span class="number">1</span>;</span><br><span class="line">    <span class="built_in">build</span>(lson);</span><br><span class="line">    <span class="built_in">build</span>(mid+<span class="number">1</span>,r,rt&lt;&lt;<span class="number">1</span>|<span class="number">1</span>);</span><br><span class="line">    <span class="built_in">push_up</span>(rt);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">upd</span><span class="params">(<span class="type">int</span> p,<span class="type">int</span> val,<span class="type">int</span> l,<span class="type">int</span> r,<span class="type">int</span> rt)</span><span class="comment">//更新</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l==r)</span><br><span class="line">    &#123;</span><br><span class="line">        sum[rt]+=val;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> mid =l+r&gt;&gt;<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(p&lt;=mid) <span class="built_in">upd</span>(p,val,lson);</span><br><span class="line">    <span class="keyword">else</span> <span class="built_in">upd</span>(p,val,rson);</span><br><span class="line">    <span class="built_in">push_up</span>(rt);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">query</span><span class="params">(<span class="type">int</span> L,<span class="type">int</span> R,<span class="type">int</span> l,<span class="type">int</span> r,<span class="type">int</span> rt)</span><span class="comment">//查询</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L&lt;=l&amp;&amp;R&gt;=r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> sum[rt];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> mid =l+r&gt;&gt;<span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> ret=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(L&lt;=mid) ret+=<span class="built_in">query</span>(L,R,lson);</span><br><span class="line">    <span class="keyword">if</span>(R&gt;mid) ret+=<span class="built_in">query</span>(L,R,rson);</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Kruskal算法并查集求最小生成树"><a href="#Kruskal算法并查集求最小生成树" class="headerlink" title="Kruskal算法并查集求最小生成树"></a>Kruskal算法并查集求最小生成树</h3><p>```cpp</p>
<h1 id="define-maxn-25005"><a href="#define-maxn-25005" class="headerlink" title="define maxn 25005"></a>define maxn 25005</h1><p>using namespace std;<br>/<em>边的定义</em>/<br>typedef struct edge<br>{<br>       int x,y;     //边的两个端点编号<br>       int cost;    //边权<br>}edge;<br>edge s[maxn];<br>int t,n,f[510];<br>/<em>边的比较函数</em>/<br>bool cmp(edge a,edge b)<br>{<br>       return a.cost<b.cost; } void init() { for(int i="1;i<=n;i++)" f[i]="i;" int find(int x) *并查集查询函数，返回x所在集合的根结点* return f[x]="=x?x:f[x]=find(f[x]);" merge(int x,int y) fx="find(x);" fy="find(y);" f[fx]="fy;" *kruskal算法求无向图的最小生成树* kruskal() sort(s+1,s+1+t,cmp); ans="0,k=0;" if(find(s[i].x)!="find(s[i].y))" merge(s[i].x,s[i].y); k++; if(k="=n-1)" ans; main() z; scanf("%d",&z); while(z--) scanf("%d",&n); x; init();t="1;" j="1;j<=n;j++)" scanf("%d",&x); if(j>i&amp;&amp;x!=0)<br>                   {<br>                       s[t].x=i;s[t].y=j;<br>                       s[t].cost=x;<br>                       t++;<br>                   }<br>               }<br>           }<br>           int res=Kruskal();<br>           printf(“%d\n”,res);<br>       }<br>}</b.cost;></p>
<p>``</p>
]]></content>
      <categories>
        <category>程序设计</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>ACM</tag>
        <tag>线段树</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer</title>
    <url>/2021/07/10/011-Vision-Transformer/</url>
    <content><![CDATA[<p>一般使用的patch_size大小是16*16，可以通过减小patch_size来完成更精细化的tokenization，但是序列的长度N=HW/P^2过长，假设p从16变为4，序列的长度会变为原来的16倍，而ViT模型的计算量与序列长度的平方成正比，计算量就会相应地变成原来的256倍</p>
<blockquote>
<p>1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下Attention结构，使得它有能力分辨不同位置的Token，这构成了相对位置编码的一般做法。<br>相对位置并没有完整建模每个输入的位置信息，而是在算Attention的时候考虑当前位置与被Attention的位置的相对距离，由于自然语言一般更依赖于相对位置，所以相对位置编码通常也有着优秀的表现。对于相对位置编码来说，它的灵活性更大，更加体现出了研究人员的“天马行空”。</p>
</blockquote>
<h2 id="2021-CoaT：Co-Scale-Conv-Attentional-ImageTransformers【开源】【分类】"><a href="#2021-CoaT：Co-Scale-Conv-Attentional-ImageTransformers【开源】【分类】" class="headerlink" title="2021-CoaT：Co-Scale Conv-Attentional ImageTransformers【开源】【分类】"></a>2021-CoaT：Co-Scale Conv-Attentional ImageTransformers【开源】【分类】</h2><p>【代码】： <a href="https://github.com/mlpc-ucsd/CoaT">https://github.com/mlpc-ucsd/CoaT</a></p>
<p>CoaT为图像Transformer提供了丰富的多尺度和上下文建模能力</p>
<p><strong>摘要</strong><br>在本文中，我们介绍了Co-scale conv-attentional image Transformers（CoaT），这是一种基于Transformer的图像分类器，配备了co-scale和conv-attentional机制。首先，co-scale机制在各个尺度上都保持了Transformers编码器分支的完整性，同时允许在不同尺度下学习的表示形式能够有效地彼此通信。我们设计了一系列的串行和并行块，以实现co-scale 注意力机制。其次，我们通过一种高效的类似于卷积的实现方式，在因式注意模块中实现相对位置嵌入公式，设计了一种factorized attention机制。 CoaT为图像Trasformer提供了丰富的多尺度和上下文建模功能。在ImageNet上，与类似大小的卷积神经网络和图像/视觉Transformer相比，相对较小的CoaT模型可获得更好的分类结果。 CoaT骨干网的有效性在目标检测和实例分割上也得到了说明，证明了其对下游计算机视觉任务的适用性。</p>
<ol>
<li><img src="https://img-blog.csdnimg.cn/img_convert/e5c90f76658dacbdda4514770a973802.png" alt="在这里插入图片描述"><br>左边是CoaT-Lite，只有串行模块<br>右边是CoaT，加上并行模块</li>
</ol>
<h2 id="2021-CPVT：Conditional-Positional-Encodings-for-Vision-Transformer【开源】【分类】【美团】"><a href="#2021-CPVT：Conditional-Positional-Encodings-for-Vision-Transformer【开源】【分类】【美团】" class="headerlink" title="2021-CPVT：Conditional Positional Encodings for Vision Transformer【开源】【分类】【美团】"></a>2021-CPVT：Conditional Positional Encodings for Vision Transformer【开源】【分类】【美团】</h2><p>又名： Do We Really Need Explicit Position Encodings for Vision Transformers<br>【代码】： <a href="https://github.com/Meituan-AutoML/CPVT">https://github.com/Meituan-AutoML/CPVT</a>   【代码是和Twins一起放出来的，这个链接没有代码】<br>成功的视觉任务位置编码应满足以下要求：</p>
<ol>
<li>是输入序列排列可变但平移不变</li>
<li>具有归纳性并且能够处理比训练期间更长的序列</li>
<li>具有一定程度能提供绝对位置的能力</li>
</ol>
<blockquote>
<p>CPVT的PEG（Positional Encoding Generator）通过卷积聚合邻域内的特征生成位置编码，且边缘的零填充对于模型知道绝对位置很重要，并且做了消融实验来验证，分析原因是因为：绝对位置信息在物体分类中起着重要作用，每个图象的类别主要由中心的对象标记，模型需要绝对位置来确定哪个补丁位于中心。</p>
</blockquote>
<p>全局平均池化GAP替换class token</p>
<h2 id="2021-CrossViT-Cross-Attention-Multi-Scale-Vision-Transformer-for-Image-Classification【民间代码】"><a href="#2021-CrossViT-Cross-Attention-Multi-Scale-Vision-Transformer-for-Image-Classification【民间代码】" class="headerlink" title="2021-CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification【民间代码】"></a>2021-CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification【民间代码】</h2><p>【代码】： <a href="https://github.com/rishikksh20/CrossViT-pytorch">https://github.com/rishikksh20/CrossViT-pytorch</a><br>yangzhi</p>
<h2 id="2021-CAT-Cross-Attention-in-Vision-Transformer【开源】【分类】"><a href="#2021-CAT-Cross-Attention-in-Vision-Transformer【开源】【分类】" class="headerlink" title="2021-CAT: Cross Attention in Vision Transformer【开源】【分类】"></a>2021-CAT: Cross Attention in Vision Transformer【开源】【分类】</h2><p>【代码】： </p>
<h2 id="2021-MSG-Transformer：Exchanging-Local-Spatial-Information-by-Manipulating-Messenger-Tokens【开源】【分类】【结果不好】"><a href="#2021-MSG-Transformer：Exchanging-Local-Spatial-Information-by-Manipulating-Messenger-Tokens【开源】【分类】【结果不好】" class="headerlink" title="2021-MSG-Transformer：Exchanging Local Spatial Information by Manipulating Messenger Tokens【开源】【分类】【结果不好】"></a>2021-MSG-Transformer：Exchanging Local Spatial Information by Manipulating Messenger Tokens【开源】【分类】【结果不好】</h2><p>【代码】： <a href="https://github.com/hustvl/MSG-Transformer">https://github.com/hustvl/MSG-Transformer</a></p>
<p><strong>摘要：</strong><br>Transformers 提供了一种设计用于视觉识别的神经网络的新方法。 与卷积网络相比，Transformers 享有<strong>在每个阶段引用全局特征的能力</strong>，但注意力模块带来更高的计算开销，阻碍了 Transformer 处理高分辨率视觉数据的应用。 本文旨在缓解效率和灵活性之间的冲突，为此我们为每个区域提出了一个专门的token作为信使（MSG）。 因此，通过<strong>操纵这些 MSG tokens，人们可以跨区域灵活地交换视觉信息</strong>，并降低计算复杂度。 然后，我们将 MSG token集成到名为 MSG-Transformer 的多尺度架构中。 在标准图像分类和目标检测中，MSG-Transformer 实现了具有竞争力的性能，并且加速了 GPU 和 CPU 上的推理。</p>
<h2 id="2021-Swin-Transformer【已开源】【分类】"><a href="#2021-Swin-Transformer【已开源】【分类】" class="headerlink" title="2021-Swin Transformer【已开源】【分类】"></a>2021-Swin Transformer【已开源】【分类】</h2><p>【代码】：<br><a href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a><br><a href="https://github.com/berniwal/swin-transformer-pytorch">https://github.com/berniwal/swin-transformer-pytorch</a></p>
<h2 id="2021-Swin-Unet【已开源】【分割】【2021CVPR】"><a href="#2021-Swin-Unet【已开源】【分割】【2021CVPR】" class="headerlink" title="2021-Swin-Unet【已开源】【分割】【2021CVPR】"></a>2021-Swin-Unet【已开源】【分割】【2021CVPR】</h2><p>【代码】： <a href="https://github.com/HuCaoFighting/Swin-Unet">https://github.com/HuCaoFighting/Swin-Unet</a></p>
<p><img src="https://img-blog.csdnimg.cn/20210715201235141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-UCTransNet-Rethinking-the-Skip-Connections-in-U-Net-from-a-Channel-wise-Perspective-with-Transformer【已开源】【分割】"><a href="#2021-UCTransNet-Rethinking-the-Skip-Connections-in-U-Net-from-a-Channel-wise-Perspective-with-Transformer【已开源】【分割】" class="headerlink" title="2021-UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer【已开源】【分割】"></a>2021-UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer【已开源】【分割】</h2><p>【代码】：<a href="https://github.com/McGregorWwww/UCTransNet">https://github.com/McGregorWwww/UCTransNet</a></p>
<h2 id="2021-CoTNet-Contextual-Transformer-Networks-for-Visual-Recognition【已开源】【分类】"><a href="#2021-CoTNet-Contextual-Transformer-Networks-for-Visual-Recognition【已开源】【分类】" class="headerlink" title="2021- CoTNet-Contextual Transformer Networks for Visual Recognition【已开源】【分类】"></a>2021- CoTNet-Contextual Transformer Networks for Visual Recognition【已开源】【分类】</h2><p>【代码】：<a href="https://github.com/JDAI-CV/CoTNet">https://github.com/JDAI-CV/CoTNet</a></p>
<p><del>(代码没太看懂)</del> </p>
<p>本文认为，Transformer的自注意力仅在空域进行信息交互，依赖于输入自身相关性通过独立的方式学习所得，而忽略了近邻间丰富的上下文信息。<br><img src="https://img-blog.csdnimg.cn/465450ac77ed4f428f0de8ed76433800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b696d5f224344f83962e5c40d04e3617.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/5e75b9874b104317941e17de7cf1580c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/dc99ecd85c1e4754a83ef17e0ca33a26.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/8f222dc07041466b9fe0422a4ac4560b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-MaskFormer-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation【已开源】【分割】"><a href="#2021-MaskFormer-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation【已开源】【分割】" class="headerlink" title="2021- MaskFormer- Per-Pixel Classification is Not All You Need for Semantic Segmentation【已开源】【分割】"></a>2021- MaskFormer- Per-Pixel Classification is Not All You Need for Semantic Segmentation【已开源】【分割】</h2><p>【代码】： <a href="https://github.com/facebookresearch/MaskFormer">https://github.com/facebookresearch/MaskFormer</a><br><img src="https://img-blog.csdnimg.cn/f9754efef4104ee39fa533b6364228b7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/d1a1fbfc70f94991afc34ac291fdc415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/ff26fa3e30c440cf9ab4b0488a1ac1fe.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>MaskFormer的结构如上图所示，主要可以分为三个部分：</p>
<p>1）pixel-level module：用来提取每个像素embedding（灰色背景部分）</p>
<p>2）transformer module：用来计算N个segment的embedding（绿色背景部分）</p>
<p>3）segmentation module：根据上面的per-pixel embedding和per-segment embedding，生成预测结果。（蓝色背景部分）</p>
<p><strong>3.3.1. Pixel-level module</strong><br>在Pixel-level module中，首先用backbone对图片的H和W进行压缩，通道维度进行提升，提取视觉特征，这一部分和正常CNN提取特征类似。然后用一个pixel Decoder去将长宽重新转换为H和W。</p>
<p><strong>3.3.2. Transformer module</strong><br>Transformer module的结构就是标准的Transformer Decoder的结构，根据视觉特征和N个可学习的query来计算输出。</p>
<p><strong>3.3.3. Segmentation module</strong><br>Segmentation  module就是一个FC的结构，后面接上softmax激活函数。它的输出就是segment的概率估计，因此根据这个概率估计和GroundTruth做分类的损失。</p>
<p>对于mask的预测，作者将per-segment embedding通过一个两层的MLP转换成了N个mask embedding。接着，作者将mask embedding和per-pixel embedding进行了点乘，后面接上了sigmoid激活函数，来获得最后mask的预测结果。 </p>
<p><strong>本文亮点总结</strong></p>
<blockquote>
<p>1.作者在这篇论文中提出，其实mask分类是非常通用的，完全可以用mask分类来统一语义分类和实例分类的范式。因此，作者提出了MaskFormer，能够将现有的per-pixel分类的模型转换为mask分类的模型。</p>
<p>2.作者通过实验表明，一个简单的mask分类模型可以优于SOTA的per-pixel分类模型，特别是在存在大量类别的情况下。本文提出的MaskFormer在全景分割任务上也保持着很强的竞争力，最重要的不需要改变模型架构、损失或训练过程。</p>
</blockquote>
<h2 id="2021-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers【已开源】【分割】"><a href="#2021-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers【已开源】【分割】" class="headerlink" title="2021- SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers【已开源】【分割】"></a>2021- SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers【已开源】【分割】</h2><p>【代码】：<a href="https://github.com/NVlabs/SegFormer">https://github.com/NVlabs/SegFormer</a></p>
<p><img src="https://img-blog.csdnimg.cn/a217a973e0bf4c459c70fc22faab7541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>感觉没啥创新</p>
<ul>
<li>去掉固定形状的位置编码，用3*3卷积作为位置信息补充</li>
<li>编码器部分每层都输出特征图传入后面（层次化）</li>
<li>Overlapped Patch Merging，卷积核大于步长实现</li>
<li>Effiicient Self-Attention，由于计算复杂度和序列长度的平方成正比，所以通过reshape和Linear操作将长度由N=H*W变为N/R，具体操作如下<br><img src="https://img-blog.csdnimg.cn/6cc02129eecc4c339b2690adb7994633.png" alt="在这里插入图片描述"></li>
</ul>
<h2 id="2021-VOLO-Vision-Outlooker-for-Visual-Recognition【已开源】【分类】"><a href="#2021-VOLO-Vision-Outlooker-for-Visual-Recognition【已开源】【分类】" class="headerlink" title="2021-VOLO: Vision Outlooker for Visual Recognition【已开源】【分类】"></a>2021-VOLO: Vision Outlooker for Visual Recognition【已开源】【分类】</h2><p>【代码】：<a href="https://github.com/sail-sg/volo">https://github.com/sail-sg/volo</a></p>
<p>和token-to-token同一个团队</p>
<h2 id="2021-HaloNet：Scaling-Local-Self-Attention-for-Parameter-Efficient-Visual-Backbones【未开源】【分类】"><a href="#2021-HaloNet：Scaling-Local-Self-Attention-for-Parameter-Efficient-Visual-Backbones【未开源】【分类】" class="headerlink" title="2021- HaloNet：Scaling Local Self-Attention for Parameter Efficient Visual Backbones【未开源】【分类】"></a>2021- HaloNet：Scaling Local Self-Attention for Parameter Efficient Visual Backbones【未开源】【分类】</h2><p>【代码】： </p>
<h2 id="2021-CoAtNet-Marrying-Convolution-and-Attention-for-All-Data-Sizes【未开源】【分类】"><a href="#2021-CoAtNet-Marrying-Convolution-and-Attention-for-All-Data-Sizes【未开源】【分类】" class="headerlink" title="2021- CoAtNet: Marrying Convolution and Attention for All Data Sizes【未开源】【分类】"></a>2021- CoAtNet: Marrying Convolution and Attention for All Data Sizes【未开源】【分类】</h2><p>【代码】：<br>【参考博客】：<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247561682&amp;idx=1&amp;sn=5562d0dde1abbc49312864f16c927e81&amp;chksm=ec1d1c2bdb6a953dd763b7e1b8b89fdc5b11cd7ff327880e71ef43b80df88d9c232c113421dd&amp;scene=178&amp;cur_album_id=1685054606675902466#rd">极客平台</a><br>CNN具有归纳偏置（inductive bias），较强的泛化能力，Transformer具有更强的学习能力。本篇文章试图将CNN和Transformer进行结合，引入CNN那种对局部信息的感知，通过这种inductive bias，使得模型在CV任务上具有更好的性能。<br>Depthwise Convolution 的表达式可以写成：<br><img src="https://img-blog.csdnimg.cn/ce4bc96a699b49c1bf8d35f96d369c4b.png" alt="在这里插入图片描述"><br>式中xi和yi表示第 i个位置的输入和输出，Li表示第 i个位置的邻域，比如3×3的邻域。<br>Self-Attention的计算主要分为三步，第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；第二步是使用一个softmax函数对这些权重进行归一化；最后将权重和相应的键值value进行加权求和得到最后的结果。<br><img src="https://img-blog.csdnimg.cn/4cb414bc0de541d7b35b0e4bd422dab4.png" alt="在这里插入图片描述"></p>
<p>Self-attention 的表达式可以写成：<br><img src="https://img-blog.csdnimg.cn/2f6e6434c70840c796c164d58bb49fea.png" alt="在这里插入图片描述"><br>他们各自的特点是：<br><img src="https://img-blog.csdnimg.cn/12a29b0ce9e84e0d8c85489ede1271c5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为了将Conv和Self-Attention的优点结合，可以将静态的全局全局和和自适应注意力矩阵相加，因此就可以表示呈下面的公式：<br><img src="https://img-blog.csdnimg.cn/d09db6f3ae5140669ce32b2612eca4a2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>本文采用yiPre，先softmax，再求和</p>
<h2 id="2021-CSWin-Transformer-A-General-Vision-Transformer-Backbone-with-Cross-Shaped-Windows【已开源】【分类】"><a href="#2021-CSWin-Transformer-A-General-Vision-Transformer-Backbone-with-Cross-Shaped-Windows【已开源】【分类】" class="headerlink" title="2021- CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows【已开源】【分类】"></a>2021- CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows【已开源】【分类】</h2><p>【代码】： <a href="https://github.com/microsoft/CSWin-Transformer">https://github.com/microsoft/CSWin-Transformer</a><br><img src="https://img-blog.csdnimg.cn/23d7b2d8572b4edfa3fec3cf07270614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b68592f6eccc4f788dc8aa743cddb84e.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/5e67c008a146498fbbeb92b7ab9a644a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为什么每个stage的sw是不一样的？</p>
<p>因为SWin每个stage之后都进行了降采样，所以越到后面的stage，特征的H和W会越来越小。作者在前面的stage中采用了较小的sw，后面的stage中采用较大的sw。</p>
<p>个人觉得这么做主要有两方面的原因：</p>
<p>1）第一，前期H和W较大，如果sw也比较大，那就会导致计算量很大，极端情况下，sw=H或者sw=W，那就相当于直接进行了全局的SA，导致计算量爆炸。因此作者在前期H、W较大的时候，用较小的sw来限制计算量；后期等H、W比较小的时候，在用比较大的sw来扩大感受野。</p>
<p>2）第二，前面的Stage是用来获取局部的attention（这里指宽度较小的十字），后面的stage用获取全局的attention。（但是个人感觉如果这么做的话，就和作者提的Motivation有点冲突了，因为其他ViT结构的由局部慢慢变成全局注意力，如果前面的stage用较小的sw，那么前期也只能捕获局部注意力（十字形的局部），所以就跟其他结构一样是从局部到全局的一个过程。</p>
<h2 id="2021-Transformer-in-Transformer【已开源】【分类】"><a href="#2021-Transformer-in-Transformer【已开源】【分类】" class="headerlink" title="2021- Transformer in Transformer【已开源】【分类】"></a>2021- Transformer in Transformer【已开源】【分类】</h2><p>【代码】：<a href="https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch">https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch</a><br><img src="https://img-blog.csdnimg.cn/a7feabd26c1f47ee84d517d0159dcab6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-PVT-Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions【已开源】【分类】"><a href="#2021-PVT-Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions【已开源】【分类】" class="headerlink" title="2021-PVT-Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions【已开源】【分类】"></a>2021-PVT-Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions【已开源】【分类】</h2><p>【代码】：<a href="https://github.com/whai362/PVT">https://github.com/whai362/PVT</a><br><img src="https://img-blog.csdnimg.cn/e18d2477184b4c308d1d075bdfd08172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/9d6e2dbcc8804876ad887425e6399500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-Twins-Revisiting-the-Design-of-Spatial-Attention-in-Vision-Transformers【已开源】【分类】【美团】"><a href="#2021-Twins-Revisiting-the-Design-of-Spatial-Attention-in-Vision-Transformers【已开源】【分类】【美团】" class="headerlink" title="2021-Twins: Revisiting the Design of Spatial Attention in Vision Transformers【已开源】【分类】【美团】"></a>2021-Twins: Revisiting the Design of Spatial Attention in Vision Transformers【已开源】【分类】【美团】</h2><p>【代码】：<a href="https://github.com/Meituan-AutoML/Twins">https://github.com/Meituan-AutoML/Twins</a><br>CPVT认为，绝对位置编码在处理不同大小的输入时会遇到困难，且绝对位置编码也会打破平移不变性。本文证明，Swin优于PVT的主要原因就是这点，如果使用适当的位置编码，PVT实际上可以实现比Swin Transformer更好的性能<br>提出了两个Transformer结构（第一个完全没创新，就是这个拼那个，第二个主要就是局部注意力做完之后接一个全局）</p>
<blockquote>
<ol>
<li>Twins-PCPVT：用CPVT的条件位置编码CPE来替换PVT中的绝对位置编码。</li>
<li>Twins-SVT：交替局部和全局注意力</li>
</ol>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/3f364b5eca5c44d0a7b658215e15dc26.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/74a79d2bc1b64b74989e382b4f4708d0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-DPT-Deformable-Patch-based-Transformer-for-Visual-Recognition【已开源】【分类】"><a href="#2021-DPT-Deformable-Patch-based-Transformer-for-Visual-Recognition【已开源】【分类】" class="headerlink" title="2021-DPT: Deformable Patch-based Transformer for Visual Recognition【已开源】【分类】"></a>2021-DPT: Deformable Patch-based Transformer for Visual Recognition【已开源】【分类】</h2><p>【代码】： <a href="https://github.com/CASIA-IVA-Lab/DPT">https://github.com/CASIA-IVA-Lab/DPT</a></p>
<p>代码缺失部分模块<br><img src="https://img-blog.csdnimg.cn/bac411d679ad4b7dacbcbeace21d3383.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASm95Y2Vfbw==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h2 id="2021-Focal-Self-attention-for-Local-Global-Interactions-in-Vision-Transformers【未开源】【分类】"><a href="#2021-Focal-Self-attention-for-Local-Global-Interactions-in-Vision-Transformers【未开源】【分类】" class="headerlink" title="2021-Focal Self-attention for Local-Global Interactions in Vision Transformers【未开源】【分类】"></a>2021-Focal Self-attention for Local-Global Interactions in Vision Transformers【未开源】【分类】</h2><p>【代码】： </p>
<h2 id="2021-Mobile-Former-Bridging-MobileNet-and-Transformer【未开源】【分类】"><a href="#2021-Mobile-Former-Bridging-MobileNet-and-Transformer【未开源】【分类】" class="headerlink" title="2021-Mobile-Former: Bridging MobileNet and Transformer【未开源】【分类】"></a>2021-Mobile-Former: Bridging MobileNet and Transformer【未开源】【分类】</h2><p>【代码】： </p>
<p>轻量级CNN和Transformer桥接，结构挺复杂新颖</p>
<h2 id="2021-MCTrans：【即将开源】【2021MICCAI】"><a href="#2021-MCTrans：【即将开源】【2021MICCAI】" class="headerlink" title="2021-MCTrans：【即将开源】【2021MICCAI】"></a>2021-MCTrans：【即将开源】【2021MICCAI】</h2><p>【代码】： </p>
<p>MCTrans 可以很容易地插入到类似 UNet 的网络中</p>
<h2 id="2021-TransClaw-U-Net-Claw-U-Net-with-Transformers-for-Medical-Image-Segmentation【未开源】【分割】"><a href="#2021-TransClaw-U-Net-Claw-U-Net-with-Transformers-for-Medical-Image-Segmentation【未开源】【分割】" class="headerlink" title="2021-TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation【未开源】【分割】"></a>2021-TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation【未开源】【分割】</h2><p>将CNN和Transformer结合起来，表现SOTA！性能优于TransUNet、AttUNet等网络，作者单位：华东师范大学, 上海交通大学等<br>由于卷积操作的局限性，往往无法准确获取长期的空间特征。因此，我们提出了一种 TransClaw U-Net 网络结构，它在编码部分结合了卷积操作和Transformer操作。卷积部分用于提取浅层空间特征，以利于上采样后图像分辨率的恢复。 Transformer 部分用于对patch 进行编码，使用self-attention 机制获取序列之间的全局信息。解码部分保留了底部上采样结构，以获得更好的细节分割性能。 Synapse Multi-organ Segmentation Datasets 上的实验结果表明。</p>
<p><img src="https://img-blog.csdnimg.cn/20210716221414746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-TransAttUnet-Multi-level-Attention-guided-U-Net-with-Transformer-for-Medical-Image-Segmentation【未开源】【分割】"><a href="#2021-TransAttUnet-Multi-level-Attention-guided-U-Net-with-Transformer-for-Medical-Image-Segmentation【未开源】【分割】" class="headerlink" title="2021-TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation【未开源】【分割】"></a>2021-TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation【未开源】【分割】</h2><p>基于Transformer的多级注意力引导U-Net<br>表现SOTA！性能优于MCTrans、FANet和ResUNet++等网络，作者单位：哈工大(深圳), 香港中文大学<br>随着深度编码器-解码器架构和大规模注释医学数据集的发展，自动医学图像分割的发展取得了很大进展。由于卷积层的堆叠和连续的采样操作，现有的标准模型不可避免地会遇到特征表示的信息衰退问题，无法完全建模全局上下文特征依赖关系。为了克服上述挑战，本文提出了一种新的基于 Transformer 的医学图像语义分割框架 TransAttUnet，其中联合设计了多级引导注意和多尺度跳过连接，以有效增强传统 U 形的功能性和灵活性。建筑学。受 Transformer 的启发，一个具有 Transformer Self Attention (TSA) 和 Global Spatial Attention (GSA) 的新型自感知注意力 (SAA) 模块被整合到 TransAttUnet 中，以有效地学习编码器特征之间的非局部交互。特别是，我们还在解码器块之间建立了额外的多尺度跳跃连接，以聚合不同的语义尺度上采样特征。通过这种方式，增强了多尺度上下文信息的表示能力以生成判别特征。受益于这些互补的组件，所提出的 TransAttUnet 可以有效缓解信息衰退问题导致的精细细节丢失，提高医学图像分析的诊断灵敏度和分割质量。对不同成像的多个医学图像分割数据集的大量实验表明，我们的方法始终优于最先进的基线。<br><img src="https://img-blog.csdnimg.cn/20210716221547822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-FFVT-Feature-Fusion-Vision-Transformer-for-Fine-Grained-Visual-Categorization【未开源】【分类】"><a href="#2021-FFVT-Feature-Fusion-Vision-Transformer-for-Fine-Grained-Visual-Categorization【未开源】【分类】" class="headerlink" title="2021-FFVT-Feature Fusion Vision Transformer for Fine-Grained Visual Categorization【未开源】【分类】"></a>2021-FFVT-Feature Fusion Vision Transformer for Fine-Grained Visual Categorization【未开源】【分类】</h2><p>用于细粒度视觉分类的特征融合视觉Transformer<br>据作者称，这是第一个探索视觉Transformer在小规模和大规模细粒度视觉分类上性能的研究，并提出一种基于纯Transformer的框架特征融合ViT：FFVT，表现SOTA！性能优于TransFG、NTS-Net等网络，单位：格里菲斯大学<br>处理细粒度视觉分类 (FGVC) 的核心是学习细微但有辨别力的特征。以前的大多数工作都是通过显式选择判别部分或通过基于 CNN 的方法集成注意力机制来实现的。 然而，这些方法增加了计算复杂度，并使模型由包含最多对象的区域主导。最近，视觉转换器（ViT）在一般图像识别任务上实现了 SOTA 性能。自注意力机制将所有patch的信息聚合并加权到分类令牌，使其非常适合 FGVC。尽管如此，深度的分类token更关注全局信息，缺乏 FGVC 必不可少的局部和低级特征。在这项工作中，我们提出了一种新颖的纯基于Transformer的框架特征融合视觉Transformer（FFVT），我们聚合来自每个Transformer层的重要标记来补偿局部、低级和中级信息。我们设计了一个称为相互注意权重选择 (MAWS) 的新型token选择模块，以有效和高效地引导网络在不引入额外参数的情况下选择判别性token。我们在三个基准测试中验证了 FFVT 的有效性，其中 FFVT 实现了最先进的性能。<br><img src="https://img-blog.csdnimg.cn/20210716221802692.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Feature fusion is exploited before the last transformer layer to aggregate the important local, low-level and middle level information from previous layers. This is implemented by replacing the inputs (exclude classification token) of the last transformer layer with the tokens selected by the MAWS Module.<br><strong>Feature Fusion Module</strong><br>MSA在深层更倾向于关注全局信息，所以提出特征融合模块来补偿局部信息。<br>具体操作：将最后一层transformer层的输入替换为前面每层Transformer经过MAWS筛选后的tokens。In this way, the class token in the last transformer layer fully interacts with the low-level, middle level and high-level features from the previous layers, enriching the local information and feature representation capability.<br><img src="https://img-blog.csdnimg.cn/20210716222815133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Our feature fusion strategy is motivated by [12]【Transfg: A transformer architecture for fine-grained recognition】 which only selects the tokens from the penultimate transformer layer, yet our method directly aggregates important information from each layer.</p>
<p><strong>Mutual AttentionWeight Selection Module 相互注意权重选择模块</strong><br>提出一个token selection方法，通过直接利用多头自注意力模块生成的attention scores<br>来选择。<br><img src="https://img-blog.csdnimg.cn/20210716223332481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>其中，aij是token i 和token j关于token i上下文中的注意得分<br>一个简单的策略是选择和分类token有更高注意得分的token，因为分类token包含丰富的类别信息。如果用这种方法的话，可以直接对a0进行排序，然后选择值最大的token。这种方法称为single attention weight selection（SAWS）。但这种方法可能会引入噪声信息因为被选择的token可能聚合了更多来自噪声patches的信息。<br>拿3个patch块的attention得分矩阵来举例：<br><img src="https://img-blog.csdnimg.cn/2021071622395455.png" alt="在这里插入图片描述"><br>Token three is selected as it has the biggest value in the attention score vector for classification token. However, token three aggregates much information from token one (the<br> maximum attention score in a3) thus may introduce noises assuming token one is a noisy token。<br>为了解决这种问题，提出一种mutual attention weight selection module，需要被选择的token和分类token在分类token的上下文和它自己的上下文中都相似。<br>将attention得分矩阵的第一列表示为b0，b0即为分类token和其他token在其他token上下文中的attention得分向量。<br>最终，mutual attention weight（MAW）mai为：<br><img src="https://img-blog.csdnimg.cn/2021071622564865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2021-TransFG-A-Transformer-Architecture-for-Fine-grained-Recognition【已开源】【分类】"><a href="#2021-TransFG-A-Transformer-Architecture-for-Fine-grained-Recognition【已开源】【分类】" class="headerlink" title="2021-TransFG: A Transformer Architecture for Fine-grained Recognition【已开源】【分类】"></a>2021-TransFG: A Transformer Architecture for Fine-grained Recognition【已开源】【分类】</h2><p>细粒度视觉分类中最重要的问题之一是准确定位解释相似子类之间细微差异的判别区域</p>
<h2 id="2021-LG-Transformer-Local-to-Global-Self-Attention-in-Vision-Transformer【未开源】【分类】"><a href="#2021-LG-Transformer-Local-to-Global-Self-Attention-in-Vision-Transformer【未开源】【分类】" class="headerlink" title="2021-LG-Transformer-Local-to-Global Self-Attention in Vision Transformer【未开源】【分类】"></a>2021-LG-Transformer-Local-to-Global Self-Attention in Vision Transformer【未开源】【分类】</h2><p>Transformer的缺点很明显：计算复杂度和输入的特征大小呈平方的关系。因此直接对整张图片进行Self-Attention是不现实的，所以，最近的一些工作（比如Swin-Transformer[1]）采用了像CNN一样的分层结构，每层施加注意力的范围只在local window上，逐渐扩大Self-Attention的感知范围。<br>作者提出，这样的方式存在一定的缺点，因为在前面几个stage中没有对global的特征进行感知，因此，作者就提出了一种多分支的Transformer设计结构，使得Transformer在每个stage中都进同时进行全局和局部的信息感知。通过引入多分支结构，使得模型在分类任务和语义分割任务上都取得了一定的性能提升。<br>希望在前面的stage中也能加入全局的信息感知，所以作者引入了多分支结构，有的分支用来捕获全局的注意力（先降采样），有的分支用来捕获局部注意力，相当于是在Transformer中采用了一个类似Inception的结构<br><img src="https://img-blog.csdnimg.cn/img_convert/60d270a3a12e2f2b6cbd564deca1c198.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/img_convert/4f61c338e01cb3b8b4340bea7b6ab6da.png" alt="在这里插入图片描述"></p>
<h2 id="2021-CCTrans-Simplifying-and-Improving-Crowd-Counting-with-Transformer"><a href="#2021-CCTrans-Simplifying-and-Improving-Crowd-Counting-with-Transformer" class="headerlink" title="2021-CCTrans: Simplifying and Improving Crowd Counting with Transformer"></a>2021-CCTrans: Simplifying and Improving Crowd Counting with Transformer</h2><ul>
<li><p>单位：哈尔滨工业大学（深圳）、美团</p>
</li>
<li><p>Arxiv: <a href="https://arxiv.org/abs/2109.14483">https://arxiv.org/abs/2109.14483</a></p>
</li>
<li><p>Github: 即将开源</p>
</li>
</ul>
<p>人群计数</p>
<p>&lt;hr style=” border:solid; width:100px; height:1px;” color=#000000 size=1”&gt;</p>
<h2 id="2021-Tokens-to-Token-ViT：Training-Vision-Transformers-from-Scratch-on-ImageNet【已开源】【分类】"><a href="#2021-Tokens-to-Token-ViT：Training-Vision-Transformers-from-Scratch-on-ImageNet【已开源】【分类】" class="headerlink" title="2021-Tokens-to-Token ViT：Training Vision Transformers from Scratch on ImageNet【已开源】【分类】"></a>2021-Tokens-to-Token ViT：Training Vision Transformers from Scratch on ImageNet【已开源】【分类】</h2><p>预训练图片大小： 384<br>【代码】： <a href="https://github.com/yitu-opensource/T2T-ViT">https://github.com/yitu-opensource/T2T-ViT</a><br>本文认为中等大小数据集上，Transformer性能低于CNN的原因有两点：</p>
<ol>
<li>ViT处理图片的方式不够好，无法建模一张图片的局部信息</li>
<li>ViT的自注意力极值的Backbone不如CNN设计的更好</li>
</ol>
<p>采用类似CNN中卷积划窗的方式，将相邻的tokens局部聚合起来，有助于建模局部特征。另外还设计了一种deep narrow（个人理解是 深+窄 的网络结构）结构，减少了运算量，并获得性能上的提升。<br>​​​​​​​​<br><img src="https://img-blog.csdnimg.cn/20210709094733464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>本文先分析了Resnet50，Vision Transformer，T2T Transformer的特征可视化。其中绿框标注的是浅层特征，如边缘，线条。红框标注全白全黑的是一些零值或过大值，对最终预测没有贡献</p>
<p>我们先从熟悉的CNN看起，在比较浅的层中，网络学习到的更多是结构信息，比如对这只小狗边缘的刻画。随着层数加深，通道数变深，特征也越来越抽象</p>
<p>再来看ViT，他每层都能很好的建模全局信息，即使是很深的层当中，也没有所谓非常抽象的东西。但它对结构信息捕捉的很少，（个人认为是没有类似CNN卷积核划窗的操作，导致对局部信息捕捉不够？）ViT的表现不尽相同，浅层特征和中层特征的冗余度很高，很多channels的可视化结果十分相似，缺乏边、角、纹理等信息）</p>
<p>Token to Token 结构</p>
<p><img src="https://img-blog.csdnimg.cn/20210709094746875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Vision Transformer是将二维图片展平成一维向量（也叫token），然后送入到Transoformer结构里。而T2T为了捕捉局部信息，它将所有的token通过reshape操作，恢复成二维，然后利用一个unfold一个划窗操作，属于一个窗口的tokens，会连接成一个更长的token，然后送入到Transformer中。</p>
<p>这样会逐渐减少token的数量，但随之而来token的长度会增加很多（因为多个tokens连接在一个token），因此后续模型也降低了维度数目，以平衡计算量。</p>
<p>经过比较作者得到两个结论：</p>
<ol>
<li>使用Deep-narrow架构，并减少embedding dimension更适合视觉Transformer，可以增加特征的丰富程度，同时减少计算量</li>
<li>SE模块的channel attention结构也可以提升ViT性能，但结果不如1</li>
</ol>
<p><strong>关于Unfold操作</strong><br>Unfold操作其实就是卷积中用到的img2col方法，将一个卷积窗口的向量，重排成一个列向量。<br><img src="https://img-blog.csdnimg.cn/5133320a402a47739bcac1b65537f58f.png" alt="在这里插入图片描述"></p>
<p>下面是一段测试代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_input = np.array([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                       [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]],</span><br><span class="line">                     [[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">                      [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">                      [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]],</span><br><span class="line">                     [[<span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>],</span><br><span class="line">                      [<span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>],</span><br><span class="line">                      [<span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>]],</span><br><span class="line">                     [[<span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>],</span><br><span class="line">                      [<span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>],</span><br><span class="line">                      [<span class="number">34</span>, <span class="number">35</span>, <span class="number">36</span>]]]]).astype(np.float32)</span><br><span class="line"></span><br><span class="line">torch_input = torch.Tensor(np_input)</span><br><span class="line"></span><br><span class="line">unfold = torch.nn.Unfold(kernel_size=<span class="number">2</span>, padding=<span class="number">0</span>, stride=<span class="number">1</span>)</span><br><span class="line">unfolded = unfold(torch_input)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(unfolded)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">         [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">         [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">13.</span>, <span class="number">14.</span>],</span><br><span class="line">         [<span class="number">11.</span>, <span class="number">12.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">16.</span>, <span class="number">17.</span>],</span><br><span class="line">         [<span class="number">14.</span>, <span class="number">15.</span>, <span class="number">17.</span>, <span class="number">18.</span>],</span><br><span class="line">         [<span class="number">19.</span>, <span class="number">20.</span>, <span class="number">22.</span>, <span class="number">23.</span>],</span><br><span class="line">         [<span class="number">20.</span>, <span class="number">21.</span>, <span class="number">23.</span>, <span class="number">24.</span>],</span><br><span class="line">         [<span class="number">22.</span>, <span class="number">23.</span>, <span class="number">25.</span>, <span class="number">26.</span>],</span><br><span class="line">         [<span class="number">23.</span>, <span class="number">24.</span>, <span class="number">26.</span>, <span class="number">27.</span>],</span><br><span class="line">         [<span class="number">28.</span>, <span class="number">29.</span>, <span class="number">31.</span>, <span class="number">32.</span>],</span><br><span class="line">         [<span class="number">29.</span>, <span class="number">30.</span>, <span class="number">32.</span>, <span class="number">33.</span>],</span><br><span class="line">         [<span class="number">31.</span>, <span class="number">32.</span>, <span class="number">34.</span>, <span class="number">35.</span>],</span><br><span class="line">         [<span class="number">32.</span>, <span class="number">33.</span>, <span class="number">35.</span>, <span class="number">36.</span>]]])</span><br></pre></td></tr></table></figure>
<p>这是对应的示意图<br><img src="https://img-blog.csdnimg.cn/20210709094809380.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29oX09PTw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>T2T ViT</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">T2T_ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, tokens_type=<span class="string">&#x27;performer&#x27;</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path_rate=<span class="number">0.</span>, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.num_features = self.embed_dim = embed_dim  <span class="comment"># num_features for consistency with other models</span></span><br><span class="line"></span><br><span class="line">        self.tokens_to_token = T2T_module(</span><br><span class="line">            img_size=img_size, tokens_type=tokens_type, in_chans=in_chans, embed_dim=embed_dim)</span><br><span class="line">        num_patches = self.tokens_to_token.num_patches</span><br><span class="line"></span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line">        self.pos_embed = nn.Parameter(data=get_sinusoid_encoding(n_position=num_patches + <span class="number">1</span>, d_hid=embed_dim),</span><br><span class="line">                                      requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)]  <span class="comment"># stochastic depth decay rule</span></span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block(</span><br><span class="line">                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line">        self.norm = norm_layer(embed_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Classifier head</span></span><br><span class="line">        self.head = nn.Linear(embed_dim, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        trunc_normal_(self.cls_token, std=<span class="number">.02</span>)</span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line">    ...忽略一些其他的方法</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        B = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = self.tokens_to_token(x)</span><br><span class="line"></span><br><span class="line">        cls_tokens = self.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">        x = self.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x = blk(x)</span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.forward_features(x)</span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>整个架构是将输入数据通过T2Tmodule，然后设立一个分类的token(cls_tokens)，将其concat到x中，并加入position embedding（这里是用一个可学习参数作为位置编码）。处理好后，输入到一个个叠起来的Transformer Block，最后取第一个token（也就是cls_tokens)，输入到分类层，得到最终结果。</p>
]]></content>
      <categories>
        <category>研究生期间</category>
      </categories>
      <tags>
        <tag>调研</tag>
        <tag>深度学习</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
</search>
